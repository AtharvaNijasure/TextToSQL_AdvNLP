{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b39341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tokenizers import AddedToken\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "from transformers.optimization import Adafactor\n",
    "from transformers.trainer_utils import set_seed\n",
    "# from utils.spider_metric.evaluator import EvaluateTool\n",
    "# from utils.load_dataset import Text2SQLDataset\n",
    "from load_dataset import Text2SQLDataset\n",
    "# from utils.text2sql_decoding_utils import decode_sqls, decode_natsqls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6862c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    " #'file path of test2sql training set.')\n",
    "# train_filepath = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/spider/baselines/seq2seq_attention_copy/data/datasets/data_final/spider_combined_train.json\"\n",
    "train_filepath = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/resdsql_pre/preprocessed_dataset.json\" \n",
    "batch_size = 1 #'input batch size.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56791bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Text2SQLDataset(\n",
    "        dir_ = train_filepath,\n",
    "        mode = \"train\")\n",
    "\n",
    "train_dataloder = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size = batch_size, \n",
    "        shuffle = True,\n",
    "        collate_fn = lambda x: x,\n",
    "        drop_last = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efd62ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloder:\n",
    "    batch_inputs = [data[0] for data in batch]\n",
    "    batch_sqls = [data[1] for data in batch]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "050db96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Find the id and cell phone of the professionals who operate two or more types of treatments.'],\n",
       " ['select professionals.professional_id , professionals.cell_number from professionals join treatments on professionals.professional_id = treatments.professional_id group by professionals.professional_id having count ( * ) >= 2'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs, batch_sqls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2849a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_len = 43\n",
    "max_decoder_len = 125\n",
    "\n",
    "max_encoder_len += 2\n",
    "max_decoder_len += 2\n",
    "\n",
    "gradient_descent_step = 4 #'perform gradient descent per \"gradient_descent_step\" steps.')\n",
    "# device = \"2\" #'the id of used GPU device.')\n",
    "learning_rate = 3e-5 #'learning rate.')\n",
    "epochs = 1 #'training epochs.')\n",
    "seed = 42 #'random seed.')\n",
    "save_path = \"models/text2sql\" #'save path of best fine-tuned text2sql model.')\n",
    "tensorboard_save_path= \"tensorboard_log/text2sql\" #'save path of tensorboard log.')\n",
    "'''\n",
    "pre-trained model name. \n",
    "options: \n",
    "    t5-base, https://huggingface.co/t5-base;\n",
    "    t5-large, https://huggingface.co/t5-large;\n",
    "    t5-3b, https://huggingface.co/t5-3b;\n",
    ")'''\n",
    "\n",
    "model_name_or_path = \"t5-small\" #\"t5-3b\",\n",
    "use_adafactor = True #'whether to use adafactor optimizer.')\n",
    "mode = \"train\" #'trian, eval or test.')\n",
    "dev_filepath = \"data/preprocessed_data/resdsql_dev.json\" #'file path of test2sql dev set.')\n",
    "original_dev_filepath = \"data/spider/dev.json\" #'file path of the original dev set (for registing evaluator).')\n",
    "db_path = \"database\" #file path of database.')\n",
    "# tables_for_natsql = \"NatSQL/NatSQLv1_6/tables_for_natsql.json\" #'file path of tables_for_natsql.json.')\n",
    "num_beams = 8 #'beam size in model.generate() function.')\n",
    "num_return_sequences = 8 #'the number of returned sequences in model.generate() function (num_return_sequences <= num_beams).')\n",
    "\n",
    "output = \"predicted_sql.txt\" #\"save file of the predicted sqls.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65f66c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "set_seed(seed)\n",
    "writer = SummaryWriter(tensorboard_save_path)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70154395",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2sql_tokenizer = T5TokenizerFast.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    add_prefix_space = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c52323c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(text2sql_tokenizer, T5TokenizerFast):\n",
    "    text2sql_tokenizer.add_tokens([AddedToken(\" <=\"), AddedToken(\" <\")])\n",
    "\n",
    "train_dataset = Text2SQLDataset(\n",
    "    dir_ = train_filepath,\n",
    "    mode = \"train\")\n",
    "\n",
    "train_dataloder = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size = batch_size, \n",
    "    shuffle = True,\n",
    "    collate_fn = lambda x: x,\n",
    "    drop_last = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64f52785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing text2sql model.\n",
      "finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"initializing text2sql model.\")\n",
    "# initialize model\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name_or_path)\n",
    "model.resize_token_embeddings(len(text2sql_tokenizer))\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "print(\"finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a0baa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use Adafactor!\n"
     ]
    }
   ],
   "source": [
    "# warm up steps (10% training step)\n",
    "num_warmup_steps = int(0.1*epochs*len(train_dataset)/batch_size)\n",
    "# total training steps\n",
    "num_training_steps = int(epochs*len(train_dataset)/batch_size)\n",
    "# save checkpoint\n",
    "num_checkpoint_steps = 500\n",
    "\n",
    "print(\"Let's use Adafactor!\")\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate, \n",
    "    scale_parameter=False, \n",
    "    relative_step=False, \n",
    "    clip_threshold = 1.0,\n",
    "    warmup_init=False)\n",
    "\n",
    "#     print(\"Let's use AdamW!\")\n",
    "#     optimizer = optim.AdamW(\n",
    "#         model.parameters(), \n",
    "#         lr = learning_rate)\n",
    "\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps = num_warmup_steps,\n",
    "    num_training_steps = num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f60436a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is epoch 1.\n",
      "batch_inputs - What is the average rank for winners in all matches?\n",
      "batch_sqls - select avg ( winner_rank ) from matches\n",
      "tokenized_inputs - {'input_ids': tensor([[  363,    19,     8,  1348, 11003,    21,  8969,    16,    66,  6407,\n",
      "            58,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "tokenized_outputs - {'input_ids': tensor([[1738,    3,    9,  208,  122,   41, 4668,  834, 6254,    3,   61,   45,\n",
      "         6407,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0]])}\n",
      "encoder_input_ids - tensor([[  363,    19,     8,  1348, 11003,    21,  8969,    16,    66,  6407,\n",
      "            58,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0]])\n",
      "encoder_input_attention_mask - tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "decoder_labels - tensor([[1738,    3,    9,  208,  122,   41, 4668,  834, 6254,    3,   61,   45,\n",
      "         6407,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0]])\n",
      "decoder_attention_mask - tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "train_step = 0\n",
    "for epoch in range(epochs):\n",
    "    print(f\"This is epoch {epoch+1}.\")\n",
    "    for idx, batch in enumerate(train_dataloder):\n",
    "        train_step += 1\n",
    "\n",
    "        batch_inputs = [data[0] for data in batch]\n",
    "        batch_sqls = [data[1] for data in batch]\n",
    "#             batch_db_ids = [data[2] for data in batch] # unused\n",
    "#             batch_tc_original = [data[3] for data in batch] # unused\n",
    "\n",
    "        if epoch == 0 and idx == 0:\n",
    "            for batch_id in range(len(batch_inputs)):\n",
    "                print(f\"batch_inputs - {batch_inputs[batch_id]}\")\n",
    "                print(f\"batch_sqls - {batch_sqls[batch_id]}\")\n",
    "#                 print(\"----------------------\")\n",
    "\n",
    "        tokenized_inputs = text2sql_tokenizer(\n",
    "            batch_inputs, \n",
    "            padding = \"max_length\",\n",
    "            return_tensors = \"pt\",\n",
    "            max_length = max_encoder_len, #512,\n",
    "            truncation = True\n",
    "        )\n",
    "\n",
    "        with text2sql_tokenizer.as_target_tokenizer():\n",
    "            tokenized_outputs = text2sql_tokenizer(\n",
    "                batch_sqls, \n",
    "                padding = \"max_length\", \n",
    "                return_tensors = 'pt',\n",
    "                max_length = max_decoder_len, #256,\n",
    "                truncation = True\n",
    "            )\n",
    "            \n",
    "        encoder_input_ids = tokenized_inputs[\"input_ids\"]\n",
    "        encoder_input_attention_mask = tokenized_inputs[\"attention_mask\"]\n",
    "        \n",
    "        decoder_labels = tokenized_outputs[\"input_ids\"]\n",
    "        # replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "        decoder_labels[decoder_labels == text2sql_tokenizer.pad_token_id] = -100\n",
    "        decoder_attention_mask = tokenized_outputs[\"attention_mask\"]\n",
    "        \n",
    "        if idx == 0:\n",
    "            print(f\"tokenized_inputs - {tokenized_inputs}\")\n",
    "            print(f\"tokenized_outputs - {tokenized_outputs}\")\n",
    "#             print(f\"encoder_input_ids - {encoder_input_ids}\")\n",
    "#             print(f\"encoder_input_attention_mask - {encoder_input_attention_mask}\")\n",
    "#             print(f\"decoder_labels - {decoder_labels}\")\n",
    "#             print(f\"decoder_attention_mask - {decoder_attention_mask}\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            encoder_input_ids = encoder_input_ids.cuda()\n",
    "            encoder_input_attention_mask = encoder_input_attention_mask.cuda()\n",
    "            decoder_labels = decoder_labels.cuda()\n",
    "            decoder_attention_mask = decoder_attention_mask.cuda()\n",
    "\n",
    "        model_outputs = model(\n",
    "            input_ids = encoder_input_ids,\n",
    "            attention_mask = encoder_input_attention_mask,\n",
    "            labels = decoder_labels,\n",
    "            decoder_attention_mask = decoder_attention_mask,\n",
    "            return_dict = True\n",
    "        )\n",
    "\n",
    "        loss = model_outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if writer is not None:\n",
    "            # record training loss (tensorboard)\n",
    "            writer.add_scalar('train loss', loss.item(), train_step)\n",
    "            # record learning rate (tensorboard)\n",
    "            writer.add_scalar('train lr', optimizer.state_dict()['param_groups'][0]['lr'], train_step)\n",
    "\n",
    "        if train_step % gradient_descent_step == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if train_step % num_checkpoint_steps == 0 and epoch >= 6:\n",
    "            print(f\"At {train_step} training step, save a checkpoint.\")\n",
    "            os.makedirs(save_path, exist_ok = True)\n",
    "            model.save_pretrained(save_directory = save_path + \"/checkpoint-{}\".format(train_step))\n",
    "            text2sql_tokenizer.save_pretrained(save_directory = save_path + \"/checkpoint-{}\".format(train_step))\n",
    "            \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ccf1b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 - ▁or\n",
      "363 - ▁What\n",
      "1 - </s>\n",
      "0 - <pad>\n",
      "58 - ?\n",
      "1738 - ▁select\n",
      "3 - ▁\n",
      "9 - a\n",
      "208 - v\n",
      "122 - g\n",
      "41 - ▁(\n",
      "4668 - ▁winner\n",
      "834 - _\n",
      "6254 - rank\n",
      "3 - ▁\n",
      "61 - )\n",
      "45 - ▁from\n",
      "6407 - ▁matches\n"
     ]
    }
   ],
   "source": [
    "for token_id in [42, 363, 1, 0, 58, 1738, 3,  9,  208,  122,   41, 4668,  834, 6254,  3,   61,   45, 6407]:\n",
    "    vocab_word = text2sql_tokenizer.convert_ids_to_tokens(token_id)\n",
    "    print(f\"{token_id} - {vocab_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22d4a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0399ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test(opt):\n",
    "    set_seed(opt.seed)\n",
    "    print(opt)\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = opt.device\n",
    "\n",
    "    # initialize tokenizer\n",
    "    tokenizer = T5TokenizerFast.from_pretrained(\n",
    "        opt.save_path,\n",
    "        add_prefix_space = True\n",
    "    )\n",
    "    \n",
    "    if isinstance(tokenizer, T5TokenizerFast):\n",
    "        tokenizer.add_tokens([AddedToken(\" <=\"), AddedToken(\" <\")])\n",
    "    \n",
    "    dev_dataset = Text2SQLDataset(\n",
    "        dir_ = opt.dev_filepath,\n",
    "        mode = opt.mode\n",
    "    )\n",
    "\n",
    "    dev_dataloder = DataLoader(\n",
    "        dev_dataset, \n",
    "        batch_size = opt.batch_size, \n",
    "        shuffle = False,\n",
    "        collate_fn = lambda x: x,\n",
    "        drop_last = False\n",
    "    )\n",
    "\n",
    "    # initialize model\n",
    "    model = T5ForConditionalGeneration.from_pretrained(opt.save_path)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    model.eval()\n",
    "    predict_sqls = []\n",
    "    for batch in tqdm(dev_dataloder):\n",
    "        batch_inputs = [data[0] for data in batch]\n",
    "        batch_db_ids = [data[1] for data in batch]\n",
    "        batch_tc_original = [data[2] for data in batch]\n",
    "\n",
    "        tokenized_inputs = tokenizer(\n",
    "            batch_inputs, \n",
    "            return_tensors=\"pt\",\n",
    "            padding = \"max_length\",\n",
    "            max_length = 512,\n",
    "            truncation = True\n",
    "        )\n",
    "        \n",
    "        encoder_input_ids = tokenized_inputs[\"input_ids\"]\n",
    "        encoder_input_attention_mask = tokenized_inputs[\"attention_mask\"]\n",
    "        if torch.cuda.is_available():\n",
    "            encoder_input_ids = encoder_input_ids.cuda()\n",
    "            encoder_input_attention_mask = encoder_input_attention_mask.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_outputs = model.generate(\n",
    "                input_ids = encoder_input_ids,\n",
    "                attention_mask = encoder_input_attention_mask,\n",
    "                max_length = 256,\n",
    "                decoder_start_token_id = model.config.decoder_start_token_id,\n",
    "                num_beams = opt.num_beams,\n",
    "                num_return_sequences = opt.num_return_sequences\n",
    "            )\n",
    "\n",
    "            model_outputs = model_outputs.view(len(batch_inputs), opt.num_return_sequences, model_outputs.shape[1])\n",
    "\n",
    "            predict_sqls += decode_sqls(\n",
    "                opt.db_path, \n",
    "                model_outputs, \n",
    "                batch_db_ids, \n",
    "                batch_inputs, \n",
    "                tokenizer, \n",
    "                batch_tc_original\n",
    "            )\n",
    "\n",
    "    new_dir = \"/\".join(opt.output.split(\"/\")[:-1]).strip()\n",
    "    if new_dir != \"\":\n",
    "        os.makedirs(new_dir, exist_ok = True)\n",
    "    \n",
    "    # save results\n",
    "    with open(opt.output, \"w\", encoding = 'utf-8') as f:\n",
    "        for pred in predict_sqls:\n",
    "            f.write(pred + \"\\n\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"Text-to-SQL inference spends {}s.\".format(end_time-start_time))\n",
    "    \n",
    "    if opt.mode == \"eval\":\n",
    "        # initialize evaluator\n",
    "        evaluator = EvaluateTool()\n",
    "        evaluator.register_golds(opt.original_dev_filepath, opt.db_path)\n",
    "        spider_metric_result = evaluator.evaluate(predict_sqls)\n",
    "        print('exact_match score: {}'.format(spider_metric_result[\"exact_match\"]))\n",
    "        print('exec score: {}'.format(spider_metric_result[\"exec\"]))\n",
    "    \n",
    "        return spider_metric_result[\"exact_match\"], spider_metric_result[\"exec\"]\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    opt = parse_option()\n",
    "    if opt.mode in [\"train\"]:\n",
    "        _train(opt)\n",
    "    elif opt.mode in [\"eval\", \"test\"]:\n",
    "        _test(opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
