{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertConfig, BertPreTrainedModel, BertTokenizer\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from model_gcn import GAT, GCN, Rel_GAT\n",
    "from model_utils import LinearAttention, DotprodAttention, RelationAttention, Highway, mask_logits\n",
    "from tree import *\n",
    "# used reference from : https://github.com/shenwzh3/RGAT-ABSA\n",
    "# inspired from https://aclanthology.org/2020.acl-main.295.pdf\n",
    "import os\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import random\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class R_GATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_relations, num_heads=1):\n",
    "        super(R_GATLayer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_relations = num_relations\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.W = nn.Parameter(torch.Tensor(num_heads, in_dim, out_dim))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "\n",
    "        self.a = nn.Parameter(torch.Tensor(num_heads, 2*out_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.a)\n",
    "\n",
    "        self.relations = nn.Parameter(torch.Tensor(num_relations, out_dim, out_dim))\n",
    "        nn.init.xavier_uniform_(self.relations)\n",
    "\n",
    "    def forward(self, x, edge_lists):\n",
    "        h = torch.matmul(x, self.W)\n",
    "\n",
    "        heads = []\n",
    "        for i in range(self.num_heads):\n",
    "            attention_input = torch.cat([h[edge_lists[:, 0]], h[edge_lists[:, 1]], self.relations[edge_lists[:, 2]]], dim=-1)\n",
    "            attention_logits = torch.matmul(attention_input, self.a[i])\n",
    "            attention_weights = F.softmax(attention_logits, dim=0)\n",
    "            head = torch.sum(attention_weights * h[edge_lists[:, 1]], dim=0)\n",
    "            heads.append(head)\n",
    "\n",
    "        output = torch.mean(torch.stack(heads), dim=0)\n",
    "\n",
    "        return output\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class R_GAT(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_relations, num_heads=1, num_layers=1):\n",
    "        super(R_GAT, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                self.layers.append(R_GATLayer(in_dim, out_dim, num_relations, num_heads))\n",
    "            else:\n",
    "                self.layers.append(R_GATLayer(out_dim, out_dim, num_relations, num_heads))\n",
    "\n",
    "    def forward(self, x, edge_lists):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.layers[i](x, edge_lists)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Aspect_Text_GAT_only(nn.Module):\n",
    "    \"\"\"\n",
    "    reshape tree in GAT only\n",
    "    \"\"\"\n",
    "    def __init__(self, args, dep_tag_num, pos_tag_num):\n",
    "        super(Aspect_Text_GAT_only, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        num_embeddings, embed_dim = args.glove_embedding.shape\n",
    "        self.embed = nn.Embedding(num_embeddings, embed_dim)\n",
    "        self.embed.weight = nn.Parameter(\n",
    "            args.glove_embedding, requires_grad=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        if args.highway:\n",
    "            self.highway = Highway(args.num_layers, args.embedding_dim)\n",
    "\n",
    "        self.bilstm = nn.LSTM(input_size=args.embedding_dim, hidden_size=args.hidden_size,\n",
    "                                  bidirectional=True, batch_first=True, num_layers=args.num_layers)\n",
    "        gcn_input_dim = args.hidden_size * 2\n",
    "\n",
    "        # if args.gat:\n",
    "        if args.gat_attention_type == 'linear':\n",
    "            self.gat = [LinearAttention(in_dim = gcn_input_dim, mem_dim = gcn_input_dim).to(args.device) for i in range(args.num_heads)] # we prefer to keep the dimension unchanged\n",
    "        elif args.gat_attention_type == 'dotprod':\n",
    "            self.gat = [DotprodAttention().to(args.device) for i in range(args.num_heads)]\n",
    "        else:\n",
    "            # reshaped gcn\n",
    "            self.gat = nn.Linear(gcn_input_dim, gcn_input_dim)\n",
    "\n",
    "\n",
    "        last_hidden_size = args.hidden_size * 2\n",
    "\n",
    "        layers = [\n",
    "            nn.Linear(last_hidden_size, args.final_hidden_size), nn.ReLU()]\n",
    "        for _ in range(args.num_mlps-1):\n",
    "            layers += [nn.Linear(args.final_hidden_size,\n",
    "                                 args.final_hidden_size), nn.ReLU()]\n",
    "        self.fcs = nn.Sequential(*layers)\n",
    "        self.fc_final = nn.Linear(args.final_hidden_size, args.num_classes)\n",
    "\n",
    "    def forward(self, sentence, aspect, pos_class, dep_tags, text_len, aspect_len, dep_rels, dep_heads, aspect_position, dep_dirs):\n",
    "        '''\n",
    "        Forward takes:\n",
    "            sentence: sentence_id of size (batch_size, text_length)\n",
    "            aspect: aspect_id of size (batch_size, aspect_length)\n",
    "            pos_class: pos_tag_id of size (batch_size, text_length)\n",
    "            dep_tags: dep_tag_id of size (batch_size, text_length)\n",
    "            text_len: (batch_size,) length of each sentence\n",
    "            aspect_len: (batch_size, ) aspect length of each sentence\n",
    "            dep_rels: (batch_size, text_length) relation\n",
    "            dep_heads: (batch_size, text_length) which node adjacent to that node\n",
    "            aspect_position: (batch_size, text_length) mask, with the position of aspect as 1 and others as 0\n",
    "            dep_dirs: (batch_size, text_length) the directions each node to the aspect\n",
    "        '''\n",
    "        fmask = (torch.zeros_like(sentence) != sentence).float()  # (Nï¼ŒL)\n",
    "        dmask = (torch.zeros_like(dep_tags) != dep_tags).float()  # (N ,L)\n",
    "\n",
    "        feature = self.embed(sentence)  # (N, L, D)\n",
    "        aspect_feature = self.embed(aspect) # (N, L', D)\n",
    "        feature = self.dropout(feature)\n",
    "        aspect_feature = self.dropout(aspect_feature)\n",
    "\n",
    "\n",
    "        if self.args.highway:\n",
    "            feature = self.highway(feature)\n",
    "            aspect_feature = self.highway(aspect_feature)\n",
    "\n",
    "        feature, _ = self.bilstm(feature) # (N,L,D)\n",
    "        aspect_feature, _ = self.bilstm(aspect_feature) #(N,L,D)\n",
    "\n",
    "        aspect_feature = aspect_feature.mean(dim = 1) # (N, D)\n",
    "\n",
    "        ############################################################################################\n",
    "\n",
    "        if self.args.gat_attention_type == 'gcn':\n",
    "            gat_out = self.gat(feature) # (N, L, D)\n",
    "            fmask = fmask.unsqueeze(2)\n",
    "            gat_out = gat_out * fmask\n",
    "            gat_out = F.relu(torch.sum(gat_out, dim = 1)) # (N, D)\n",
    "\n",
    "        else:\n",
    "            gat_out = [g(feature, aspect_feature, fmask).unsqueeze(1) for g in self.gat]\n",
    "            gat_out = torch.cat(gat_out, dim=1)\n",
    "            gat_out = gat_out.mean(dim=1)\n",
    "\n",
    "        feature_out = gat_out # (N, D')\n",
    "        # feature_out = gat_out\n",
    "        #############################################################################################\n",
    "        x = self.dropout(feature_out)\n",
    "        x = self.fcs(x)\n",
    "        logit = self.fc_final(x)\n",
    "        return logit\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get dataset\n",
    "from load_dataset import Text2SQLDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_filepath = \"../data/resdsql_pre/preprocessed_dataset_test.json\"\n",
    "batch_size = 2 #'input batch size.')\n",
    "\n",
    "train_dataset = Text2SQLDataset(\n",
    "        dir_ = train_filepath,\n",
    "        mode = \"train\")\n",
    "\n",
    "train_dataloder = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        collate_fn = lambda x: x,\n",
    "        drop_last = True\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_bert_optimizer(args, model):\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(\n",
    "            nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                      lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    # scheduler = WarmupLinearSchedule(\n",
    "    #     optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
    "    return optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "\n",
    "def train(args, train_dataset, model, test_dataset):\n",
    "    '''Train the model'''\n",
    "    tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler,\n",
    "                                  batch_size=args.train_batch_size)\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (\n",
    "            len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(\n",
    "            train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "\n",
    "    if args.embedding_type == 'bert':\n",
    "        optimizer = get_bert_optimizer(args, model)\n",
    "    else:\n",
    "        parameters = filter(lambda param: param.requires_grad, model.parameters())\n",
    "        optimizer = torch.optim.Adam(parameters, lr=args.learning_rate)\n",
    "\n",
    "    # Train\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\",\n",
    "                args.per_gpu_train_batch_size)\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\",\n",
    "                args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    all_eval_results = []\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "    set_seed(args)\n",
    "    for _ in train_iterator:\n",
    "        # epoch_iterator = tqdm(train_dataloader, desc='Iteration')\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "            inputs, labels = get_input_from_batch(args, batch)\n",
    "            logit = model(**inputs)\n",
    "            loss = F.cross_entropy(logit, labels)\n",
    "\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                # scheduler.step()  # Update learning rate schedule\n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                # Log metrics\n",
    "                if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    results, eval_loss = evaluate(args, test_dataset, model)\n",
    "                    all_eval_results.append(results)\n",
    "                    for key, value in results.items():\n",
    "                        tb_writer.add_scalar(\n",
    "                            'eval_{}'.format(key), value, global_step)\n",
    "                    tb_writer.add_scalar('eval_loss', eval_loss, global_step)\n",
    "                    # tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\n",
    "                        'train_loss', (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                # Save model checkpoint\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                # epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            # epoch_iterator.close()\n",
    "            break\n",
    "\n",
    "    tb_writer.close()\n",
    "    return global_step, tr_loss/global_step, all_eval_results\n",
    "\n",
    "def evaluate(args, eval_dataset, model):\n",
    "    results = {}\n",
    "\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    # collate_fn = get_collate_fn(args)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,\n",
    "                                 batch_size=args.eval_batch_size) #,\n",
    "                                 # collate_fn=collate_fn)\n",
    "\n",
    "    # Eval\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    for batch in eval_dataloader:\n",
    "    # for batch in tqdm(eval_dataloader, desc='Evaluating'):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = get_input_from_batch(args, batch)\n",
    "\n",
    "            logits = model(**inputs)\n",
    "            tmp_eval_loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = labels.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(\n",
    "                out_label_ids, labels.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    # print(preds)\n",
    "    result = compute_metrics(preds, out_label_ids)\n",
    "    results.update(result)\n",
    "\n",
    "    output_eval_file = os.path.join(args.output_dir, 'eval_RGAT_results.txt')\n",
    "    with open(output_eval_file, 'a+') as writer:\n",
    "        logger.info('***** Eval results *****')\n",
    "        logger.info(\"  eval loss: %s\", str(eval_loss))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"  %s = %s\\n\" % (key, str(result[key])))\n",
    "            writer.write('\\n')\n",
    "        writer.write('\\n')\n",
    "    return results, eval_loss\n",
    "\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "def acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds, average='macro')\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    return acc_and_f1(preds, labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Load datasets and vocabs\n",
    "# train_dataset, test_dataset, word_vocab, dep_tag_vocab, pos_tag_vocab\n",
    "# Build Model\n",
    "model = Aspect_Text_GAT_only(args, dep_tag_vocab['len'], pos_tag_vocab['len'])\n",
    "# if args.pure_bert:\n",
    "#     model = Pure_Bert(args)\n",
    "# elif args.gat_bert:\n",
    "#     model = Aspect_Bert_GAT(args, dep_tag_vocab['len'], pos_tag_vocab['len'])  # R-GAT + Bert\n",
    "# elif args.gat_our:\n",
    "#     model = Aspect_Text_GAT_ours(args, dep_tag_vocab['len'], pos_tag_vocab['len']) # R-GAT with reshaped tree\n",
    "# else:\n",
    "#     model = Aspect_Text_GAT_only(args, dep_tag_vocab['len'], pos_tag_vocab['len'])  # original GAT with reshaped tree\n",
    "\n",
    "model.to(device)\n",
    "# Train\n",
    "_, _,  all_eval_results = train(args, train_dataset, model, test_dataset)\n",
    "\n",
    "if len(all_eval_results):\n",
    "    best_eval_result = max(all_eval_results, key=lambda x: x['acc'])\n",
    "    for key in sorted(best_eval_result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(best_eval_result[key]))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
