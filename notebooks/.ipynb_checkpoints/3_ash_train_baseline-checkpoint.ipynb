{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf3fefe7",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ce0231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, errno\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from utils import save_checkpoint, load_checkpoint\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "# from load_dataset import Text2SQLDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ffed02",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd13c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train encoder input - (43, 6304)\n",
      "Train decoder input - (129, 6304)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 5938,\n",
       " 'max_encoder_len': 43,\n",
       " 'max_decoder_len': 127,\n",
       " 'pad_idx': 1462,\n",
       " 'sos_idx': 1463,\n",
       " 'eos_idx': 1461}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target_folder = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/LSTM_encoder_decoder/data/data_final_processed_v2\"\n",
    "target_folder = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/baseline/training_data\"\n",
    "\n",
    "## GET DATA\n",
    "#sample data for checking network\n",
    "fol1 = 'train'\n",
    "data_t = 'encode'\n",
    "X_train_np = np.load(os.path.join(target_folder, fol1, f\"{fol1}_{data_t}.npy\"))\n",
    "# X_train_np = X_train_np[:5]\n",
    "X_train_np = X_train_np.transpose(1,0)\n",
    "# train_input = np.expand_dims(train_input, axis=-1) \n",
    "print(f'Train encoder input - {X_train_np.shape}')\n",
    "\n",
    "\n",
    "#sample data for checking network\n",
    "fol1 = 'train'\n",
    "data_t = 'decode'\n",
    "Y_train_np = np.load(os.path.join(target_folder, fol1, f\"{fol1}_{data_t}.npy\"))\n",
    "# Y_train_np = Y_train_np[:5]\n",
    "Y_train_np = Y_train_np.transpose(1,0)\n",
    "# train_output = np.expand_dims(train_output, axis=-1) \n",
    "print(f'Train decoder input - {Y_train_np.shape}')\n",
    "\n",
    "with open(os.path.join(target_folder, 'data_info.json'), 'r') as fp:\n",
    "    data_info = json.load(fp)\n",
    "    \n",
    "pad_idx = data_info['pad_idx']\n",
    "sos_idx = data_info['sos_idx']\n",
    "vocab_size = data_info['vocab_size']\n",
    "    \n",
    "data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d0dd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([43, 6304]), torch.Size([129, 6304]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert numpy array to tensors\n",
    "X_train = torch.from_numpy(X_train_np).type(torch.int64) #torch.int64, torch.Tensor\n",
    "Y_train = torch.from_numpy(Y_train_np).type(torch.int64)\n",
    "\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6ab015",
   "metadata": {},
   "source": [
    "## Build Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24c30349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR PRINTING INTERMEDIATE TORCH SIZES\n",
    "DEBUG_FLAG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "205cf51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bilstm_encoder(nn.Module):\n",
    "    ''' Encodes time-series sequence '''\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, emb_size, num_layers = 1, dropout = 0):\n",
    "        \n",
    "        '''\n",
    "        : param input_size:     the number of features in the input X, eg: word embeddings\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        : param num_layers:     number of recurrent layers (i.e., 2 means there are\n",
    "        :                       2 stacked LSTMs)\n",
    "        '''\n",
    "        \n",
    "        super(bilstm_encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Encoder: input_size {input_size} - hidden_size {hidden_size} - emb_size {emb_size}\")\n",
    "\n",
    "        # define embeddings\n",
    "        self.embeddings = nn.Embedding(input_size, emb_size)\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size = emb_size,\n",
    "                            hidden_size = hidden_size,\n",
    "                            num_layers = num_layers,\n",
    "                            bidirectional = True,\n",
    "                            dropout = dropout)\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        \n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch) #, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence;\n",
    "        :                              hidden gives the hidden state and cell state for the last\n",
    "        :                              element in the sequence \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        embedded = self.embeddings(x_input)\n",
    "        # embedded size: (seq_len, batch_size, embedding_size)\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Encoder embedded size - {type(embedded)} - {embedded.shape}\")\n",
    "#         embedded = embedded.view(1, 1, -1)\n",
    "#         print(f\"Encoder embedded size - {type(embedded)} - {embedded.shape}\")\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(embedded)\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Encoder hidden_state size - {type(self.hidden)} - {self.hidden[0].shape}\")\n",
    "        # lstm_out, self.hidden = self.lstm(x_input.view(x_input.shape[0], x_input.shape[1], self.input_size))\n",
    "        \n",
    "        return lstm_out, self.hidden     \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        '''\n",
    "        initialize hidden state\n",
    "        : param batch_size:    x_input.shape[1]\n",
    "        : return:              zeroed hidden state and cell state \n",
    "        '''\n",
    "        \n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1333309f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([43, 6304, 60]),\n",
       " torch.Size([2, 6304, 30]),\n",
       " torch.Size([2, 6304, 30]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = bilstm_encoder(vocab_size, 30, 20)\n",
    "out, enc_hidden_state = enc.forward(X_train)\n",
    "out.shape, enc_hidden_state[0].shape, enc_hidden_state[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d18bead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No bi - (torch.Size([43, 5, 30]), torch.Size([1, 5, 30]), torch.Size([1, 5, 30]))\n",
    "# With bi - (torch.Size([43, 5, 60]), torch.Size([2, 5, 30]), torch.Size([2, 5, 30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d108575",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bilstm_decoder(nn.Module):\n",
    "    ''' Decodes hidden state output by encoder '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, emb_size, output_size, num_layers = 2, dropout = 0):\n",
    "\n",
    "        '''\n",
    "        : param input_size:     the number of features in the input X\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        : param num_layers:     number of recurrent layers (i.e., 2 means there are\n",
    "        :                       2 stacked LSTMs)\n",
    "        '''\n",
    "        \n",
    "        super(bilstm_decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Encoder: input_size {input_size} - hidden_size {hidden_size} - emb_size {emb_size} - output_size {output_size}\")\n",
    "        \n",
    "        # define embeddings\n",
    "        self.embeddings = nn.Embedding(input_size, emb_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = emb_size,\n",
    "                            hidden_size = hidden_size,\n",
    "                            num_layers = num_layers,\n",
    "                            bidirectional = False,\n",
    "                            dropout = dropout)\n",
    "        # bi is true\n",
    "        # self.linear = nn.Linear(2*hidden_size, output_size)\n",
    "        # num_layers = 1\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x_input, encoder_hidden_states):\n",
    "        \n",
    "        '''        \n",
    "        : param x_input:                    should be 2D (1, batch_size) #, input_size)\n",
    "        : param encoder_hidden_states:      hidden states\n",
    "        : return output, hidden:            output gives all the hidden states in the sequence;\n",
    "        :                                   hidden gives the hidden state and cell state for the last\n",
    "        :                                   element in the sequence \n",
    " \n",
    "        '''\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Decoder x_input size - {x_input.shape}\")\n",
    "        x_input = x_input.unsqueeze(0)\n",
    "        # x_input size: (1, batch_size)\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Decoder x_input size - {x_input.shape}\")\n",
    "        \n",
    "        embedded = self.embeddings(x_input)\n",
    "        # embedded size: (1, batch_size, embedding_size)\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Decoder embedded size - {embedded.shape}\")\n",
    "            print(f\"Decoder encoder_hidden_states size - {encoder_hidden_states[0].shape}\")\n",
    "            \n",
    "        lstm_out, self.hidden = self.lstm(embedded, encoder_hidden_states)\n",
    "        # lstm_out size: (1, batch_size, hidden_size)\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Decoder lstm_out size - {lstm_out.shape}\")\n",
    "            print(f\"Decoder hidden size - {self.hidden[0].shape}\")\n",
    "        \n",
    "        lstm_out = lstm_out.squeeze(0)\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Decoder lstm_out size - {lstm_out.shape}\")\n",
    "        output = self.linear(lstm_out) \n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Decoder output size - {output.shape}\")\n",
    "        # output size: (1, batch_size, vocab_size)\n",
    "        \n",
    "#         output = output.squeeze(0)\n",
    "#         print(f\"Decoder output size - {output.shape}\")\n",
    "        # output size: (batch_size, vocab_size)\n",
    "        \n",
    "        return output, self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "723ed77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6304, 5938]),\n",
       " torch.Size([2, 6304, 30]),\n",
       " torch.Size([2, 6304, 30]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = bilstm_decoder(vocab_size, 30, 20, vocab_size)\n",
    "out, hidden_state = dec.forward(Y_train[0], enc_hidden_state)\n",
    "out.shape, hidden_state[0].shape, hidden_state[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39d0dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e705f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_seq2seq(nn.Module):\n",
    "    ''' train LSTM encoder-decoder and make predictions '''\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "\n",
    "        '''\n",
    "        : param input_size:     the number of expected features in the input X\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        '''\n",
    "\n",
    "        super(lstm_seq2seq, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "\n",
    "    def forward(self, source, target, target_vocab_size, teacher_force_ratio = 0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        \n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "        \n",
    "        # encoder outputs\n",
    "        encoder_output, encoder_hidden = self.encoder.forward(source)\n",
    "        \n",
    "        # Grab start token\n",
    "        x = target[0]\n",
    "#         print(f\"seq2seq x size - {x.shape}\")\n",
    "        \n",
    "        for t in range(1, target_len):\n",
    "            # decoder outputs\n",
    "            decoder_output, decoder_hidden = self.decoder(x, encoder_hidden)\n",
    "            \n",
    "            outputs[t] = decoder_output\n",
    "            # output size: (N, vocab_size)\n",
    "            \n",
    "            if DEBUG_FLAG: print(f\"seq2seq decoder_output size - {decoder_output.shape}\")\n",
    "            best_guess = decoder_output.argmax(1)\n",
    "            \n",
    "            if DEBUG_FLAG: print(f\"seq2seq best_guess size - {best_guess.shape} - {best_guess}\")\n",
    "            \n",
    "            if DEBUG_FLAG: print(f\"seq2seq target size - {target[t].shape} - {target[t]}\")\n",
    "            \n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "            \n",
    "        return outputs\n",
    "    \n",
    "    def predict(self, source, target_len, target_vocab_size):\n",
    "        \n",
    "        target_len = target_len+2\n",
    "        batch_size = source.shape[1]\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "\n",
    "        # encoder outputs\n",
    "        encoder_output, encoder_hidden = self.encoder.forward(source)\n",
    "\n",
    "        # Grab start token\n",
    "        x = torch.from_numpy(np.array([sos_idx]*batch_size))\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # decoder outputs\n",
    "            decoder_output, decoder_hidden = self.decoder(x, encoder_hidden)\n",
    "\n",
    "            outputs[t] = decoder_output\n",
    "            # output size: (N, vocab_size)\n",
    "\n",
    "            best_guess = decoder_output.argmax(1)\n",
    "            x = best_guess\n",
    "#             outputs[t] = best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd08025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, emb_size, hidden_size, vocab_size, load_model = False,\n",
    "         num_epochs = 2, lr = 0.001, batch_size = 5):\n",
    "    \n",
    "    input_size_encoder = vocab_size             # german\n",
    "    input_size_decoder = vocab_size             # english\n",
    "    output_size = vocab_size                    # english\n",
    "\n",
    "    encoder_embedding_size = emb_size\n",
    "    decoder_embedding_size = emb_size\n",
    "\n",
    "    # TENSORBOARD\n",
    "    writer = SummaryWriter(f'runs/loss_plot')\n",
    "    step = 0\n",
    "\n",
    "    encoder_net = bilstm_encoder(input_size_encoder, hidden_size, emb_size).to(device)\n",
    "    decoder_net = bilstm_decoder(input_size_decoder, hidden_size, emb_size, output_size).to(device)\n",
    "    \n",
    "    model = lstm_seq2seq(encoder_net, decoder_net).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "    \n",
    "#     if load_model: load_checkpoint(torch.load('my_checkpoint.pth.ptar'), model, optimizer)\n",
    "        \n",
    "    # calculate number of batch iterations\n",
    "    n_batches = int(input_tensor.shape[1] / batch_size)\n",
    "    print(f\"Number of batches - {n_batches}\")\n",
    "    \n",
    "    # initialize array of losses \n",
    "    losses = np.full(num_epochs, np.nan)\n",
    "\n",
    "#     with trange(num_epochs) as tr:\n",
    "#         for it in tr:\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "\n",
    "    #         checkpoint = {'state_dict': model.state_dict(),\n",
    "    #                       'optimizer': optimizer.state_dict()}\n",
    "    #         save_checkpoint(checkpoint)\n",
    "\n",
    "        batch_loss = 0\n",
    "        \n",
    "        with trange(n_batches) as tr:\n",
    "            for b in tr:\n",
    "#         for b in trange(n_batches):\n",
    "            \n",
    "\n",
    "#         for b in range(n_batches):\n",
    "#             for it in tr:\n",
    "                \n",
    "#         with trange(n_batches) as tr:\n",
    "#             for it in tr:\n",
    "# #                 print(it)\n",
    "                \n",
    "#                 b = it\n",
    "        \n",
    "                # select data \n",
    "                inp_data = input_tensor[:, b*batch_size : (b+1)*batch_size] #, :]\n",
    "                target = target_tensor[:, b*batch_size : (b+1)*batch_size] #, :]\n",
    "\n",
    "    #             if step < 1:\n",
    "    #                 print(f\"batch_size - {b*batch_size} - {(b+1)*batch_size}\")\n",
    "    #                 print(f\"inp_data 0 - {inp_data}\")\n",
    "\n",
    "    #                 if step < 2:\n",
    "    #                     print(f\"inp_data 0 - {inp_data.shape}\")\n",
    "    #                     print(f\"target 0 - {target.shape}\")\n",
    "\n",
    "                output = model.forward(inp_data, target, output_size)\n",
    "                # output shape: (target_len, batch_size, output_dim)\n",
    "\n",
    "    #             if step < 1:\n",
    "    #                 print(f\"output size before reshape - {output.shape}\")\n",
    "    #                 print(f\"target size before reshape - {target.shape}\")\n",
    "\n",
    "                output = output[1:].reshape(-1, output.shape[2])\n",
    "                target = target[1:].reshape(-1)\n",
    "\n",
    "    #             if step < 1:\n",
    "    #                 print(f\"output size after reshape - {output.shape}\")\n",
    "    #                 print(f\"target size after reshape - {target.shape}\")\n",
    "\n",
    "    #                 output = output.argmax(2)\n",
    "    #                 if step < 2:\n",
    "    #                     print(f\"output 3 - {output.shape} - {type(output)} - {output[:5]}\")\n",
    "    #                     print(f\"target 3 - {target.shape} - {type(target)} - {target[:5]}\")\n",
    "\n",
    "                # zero the gradient\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # compute the loss\n",
    "                loss = criterion(output, target)\n",
    "                batch_loss += loss.item()\n",
    "\n",
    "                # backpropagation\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1) # for healthy gradients\n",
    "                optimizer.step()\n",
    "\n",
    "                writer.add_scalar('Training loss', loss, global_step=step)\n",
    "                step += 1\n",
    "\n",
    "                # progress bar \n",
    "                tr.set_postfix({\"epoch_num\":epoch+1, \"loss\":f\"{batch_loss:.3f}\"})\n",
    "                \n",
    "        # loss for epoch \n",
    "        batch_loss /= n_batches \n",
    "        losses[epoch] = batch_loss\n",
    "            \n",
    "#             # progress bar \n",
    "#             tr.set_postfix(loss=\"{0:.3f}\".format(batch_loss))\n",
    "\n",
    "\n",
    "#       # save models\n",
    "        if (epoch > 20 and epoch % 10 == 0):\n",
    "            torch.save(model, f\"model_{epoch}\")\n",
    "            \n",
    "    return losses, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1fa70ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches - 1260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▍                                                                                                                                          | 13/1260 [00:03<05:05,  4.09it/s, epoch_num=1, loss=112.597]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 96\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, emb_size, hidden_size, vocab_size, load_model, num_epochs, lr, batch_size)\u001b[0m\n\u001b[1;32m     93\u001b[0m batch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# backpropagation\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# for healthy gradients\u001b[39;00m\n\u001b[1;32m     98\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Aishwarya/Learning/venv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Aishwarya/Learning/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss, model = train(X_train, Y_train, emb_size=20, hidden_size=10, vocab_size=vocab_size,\n",
    "            num_epochs = 1, lr = 0.001, batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8769fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b9eec09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1926.47it/s, loss=0.100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_loss = 0\n",
    "# for b in trange(10):\n",
    "with trange(10) as tr:\n",
    "    for b in tr:\n",
    "        print(b)\n",
    "        batch_loss += 0.01\n",
    "        tr.set_postfix(loss=\"{0:.3f}\".format(batch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb23767e",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a394b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class must be defined somewhere\n",
    "model2 = torch.load(\"models/model_1\")\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a8b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tr in trange(10):\n",
    "#     print(tr)\n",
    "    \n",
    "with trange(10) as tr:\n",
    "    for it in tr:\n",
    "        print(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e71e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model2.predict(X_val, max_decoder_len, vocab_size)\n",
    "X_val.shape, outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0244ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(target_folder, 'idx_to_vocab.json'), 'r') as fp:\n",
    "    idx_to_vocab = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d3654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_vocab['3643'] ,idx_to_vocab['3438']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6f1d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train[:,0], train_output[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38517662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da25446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d146b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "input, target, input.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ebb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac96a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdda92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = outputs.argmax(2)\n",
    "outputs.shape, outputs[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418046c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e703d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_folder = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/LSTM_encoder_decoder/data/data_final_processed_v2\"\n",
    "\n",
    "## GET DATA\n",
    "#sample data for checking network\n",
    "fol1 = 'dev'\n",
    "data_t = 'encode'\n",
    "X_val_np = np.load(os.path.join(target_folder, fol1, f\"{fol1}_{data_t}.npy\"))\n",
    "X_val_np = X_val_np[:5]\n",
    "X_val_np = X_val_np.transpose(1,0)\n",
    "# train_input = np.expand_dims(train_input, axis=-1) \n",
    "print(f'Val encoder input - {X_val_np.shape}')\n",
    "\n",
    "\n",
    "#sample data for checking network\n",
    "fol1 = 'dev'\n",
    "data_t = 'decode'\n",
    "Y_val_np = np.load(os.path.join(target_folder, fol1, f\"{fol1}_{data_t}.npy\"))\n",
    "Y_val_np = Y_val_np[:5]\n",
    "Y_val_np = Y_val_np.transpose(1,0)\n",
    "# train_output = np.expand_dims(train_output, axis=-1) \n",
    "print(f'Val decoder input - {Y_val_np.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array to tensors\n",
    "X_val = torch.from_numpy(X_val_np).type(torch.int64) #torch.int64, torch.Tensor\n",
    "Y_val = torch.from_numpy(Y_val_np).type(torch.int64)\n",
    "\n",
    "X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6d5882",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_decoder_len = data_info['max_decoder_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab5fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
