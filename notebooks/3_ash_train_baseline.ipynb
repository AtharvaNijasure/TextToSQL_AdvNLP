{"cells":[{"cell_type":"code","source":["# MOUNTING GOOGLE DRIVE\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.getcwd()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"QSbW-vTgDaGN","executionInfo":{"status":"ok","timestamp":1683646908203,"user_tz":240,"elapsed":17765,"user":{"displayName":"Aishwarya Malgonde","userId":"02733017902828562032"}},"outputId":"d1d5c203-8fb2-44d8-f87c-74b84cc27735"},"id":"QSbW-vTgDaGN","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["wd = '/content/drive/MyDrive/CS 685/cs685_project/notebooks'\n","print(os.listdir(wd))\n","os.chdir(wd)\n","os.getcwd()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"4NAD9rIfD7XL","executionInfo":{"status":"ok","timestamp":1683646941546,"user_tz":240,"elapsed":832,"user":{"displayName":"Aishwarya Malgonde","userId":"02733017902828562032"}},"outputId":"9a190f82-eab9-40cb-deef-d8e31a7184f4"},"id":"4NAD9rIfD7XL","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['3_ash_train_baseline.ipynb', 'baseline_1.ipynb', 'Untitled0.ipynb', 'check_spider_data.ipynb', 'runs', 'model_0.001_2_0']\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/CS 685/cs685_project/notebooks'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","id":"bf3fefe7","metadata":{"id":"bf3fefe7"},"source":["## Train model"]},{"cell_type":"code","execution_count":3,"id":"49ce0231","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"49ce0231","executionInfo":{"status":"ok","timestamp":1683646950573,"user_tz":240,"elapsed":7151,"user":{"displayName":"Aishwarya Malgonde","userId":"02733017902828562032"}},"outputId":"e15ce96a-5cfa-43ad-bd7e-07d41aae9813"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":3}],"source":["import os, errno\n","import sys\n","import json\n","import random\n","import numpy as np\n","from tqdm import trange\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","\n","from torch.utils.tensorboard import SummaryWriter\n","# from utils import save_checkpoint, load_checkpoint\n","\n","# from torch.utils.data import DataLoader\n","# from load_dataset import Text2SQLDataset\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"markdown","id":"a9ffed02","metadata":{"id":"a9ffed02"},"source":["## Read data"]},{"cell_type":"code","execution_count":25,"id":"bbd13c2d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbd13c2d","executionInfo":{"status":"ok","timestamp":1683647494291,"user_tz":240,"elapsed":3,"user":{"displayName":"Aishwarya Malgonde","userId":"02733017902828562032"}},"outputId":"ca812b89-0519-42ee-c0f5-0f612440bea3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train encoder input - (43, 6304)\n","Train decoder input - (129, 6304)\n"]},{"output_type":"execute_result","data":{"text/plain":["{'vocab_size': 5938,\n"," 'max_encoder_len': 43,\n"," 'max_decoder_len': 127,\n"," 'pad_idx': 1462,\n"," 'sos_idx': 1463,\n"," 'eos_idx': 1461}"]},"metadata":{},"execution_count":25}],"source":["\n","# local\n","# target_folder = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/LSTM_encoder_decoder/data/data_final_processed_v2\"\n","# target_folder = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/baseline/training_data\"\n","\n","# for colab\n","target_folder = \"../data/baseline/training_data\"\n","\n","## GET DATA\n","#sample data for checking network\n","fol1 = 'train'\n","data_t = 'encode'\n","X_train_np = np.load(os.path.join(target_folder, fol1, f\"{fol1}_{data_t}.npy\"))\n","# X_train_np = X_train_np[:5]\n","X_train_np = X_train_np.transpose(1,0)\n","# train_input = np.expand_dims(train_input, axis=-1) \n","print(f'Train encoder input - {X_train_np.shape}')\n","\n","\n","#sample data for checking network\n","fol1 = 'train'\n","data_t = 'decode'\n","Y_train_np = np.load(os.path.join(target_folder, fol1, f\"{fol1}_{data_t}.npy\"))\n","# Y_train_np = Y_train_np[:5]\n","Y_train_np = Y_train_np.transpose(1,0)\n","# train_output = np.expand_dims(train_output, axis=-1) \n","print(f'Train decoder input - {Y_train_np.shape}')\n","\n","with open(os.path.join(target_folder, 'data_info.json'), 'r') as fp:\n","    data_info = json.load(fp)\n","    \n","pad_idx = data_info['pad_idx']\n","sos_idx = data_info['sos_idx']\n","vocab_size = data_info['vocab_size']\n","    \n","data_info"]},{"cell_type":"code","execution_count":26,"id":"15d0dd19","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"15d0dd19","executionInfo":{"status":"ok","timestamp":1683647495355,"user_tz":240,"elapsed":4,"user":{"displayName":"Aishwarya Malgonde","userId":"02733017902828562032"}},"outputId":"371d5311-845f-4905-ad13-d72e7343904d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([43, 6304]), torch.Size([129, 6304]))"]},"metadata":{},"execution_count":26}],"source":["# convert numpy array to tensors\n","X_train = torch.from_numpy(X_train_np).type(torch.int64) #torch.int64, torch.Tensor\n","Y_train = torch.from_numpy(Y_train_np).type(torch.int64)\n","\n","X_train.shape, Y_train.shape"]},{"cell_type":"markdown","id":"1a6ab015","metadata":{"id":"1a6ab015"},"source":["## Build Network"]},{"cell_type":"code","execution_count":6,"id":"24c30349","metadata":{"id":"24c30349","executionInfo":{"status":"ok","timestamp":1683646952923,"user_tz":240,"elapsed":4,"user":{"displayName":"Aishwarya Malgonde","userId":"02733017902828562032"}}},"outputs":[],"source":["# FOR PRINTING INTERMEDIATE TORCH SIZES\n","DEBUG_FLAG = False"]},{"cell_type":"code","execution_count":7,"id":"205cf51e","metadata":{"id":"205cf51e","executionInfo":{"status":"ok","timestamp":1683646952923,"user_tz":240,"elapsed":4,"user":{"displayName":"Aishwarya Malgonde","userId":"02733017902828562032"}}},"outputs":[],"source":["class bilstm_encoder(nn.Module):\n","    ''' Encodes time-series sequence '''\n","\n","    def __init__(self, input_size, hidden_size, emb_size, num_layers = 1, dropout = 0):\n","        \n","        '''\n","        : param input_size:     the number of features in the input X, eg: word embeddings\n","        : param hidden_size:    the number of features in the hidden state h\n","        : param num_layers:     number of recurrent layers (i.e., 2 means there are\n","        :                       2 stacked LSTMs)\n","        '''\n","        \n","        super(bilstm_encoder, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        \n","        if DEBUG_FLAG:\n","            print(f\"Encoder: input_size {input_size} - hidden_size {hidden_size} - emb_size {emb_size}\")\n","\n","        # define embeddings\n","        self.embeddings = nn.Embedding(input_size, emb_size)\n","\n","        # define LSTM layer\n","        self.lstm = nn.LSTM(input_size = emb_size,\n","                            hidden_size = hidden_size,\n","                            num_layers = num_layers,\n","                            bidirectional = True,\n","                            dropout = dropout)\n","\n","    def forward(self, x_input):\n","        \n","        '''\n","        : param x_input:               input of shape (seq_len, # in batch) #, input_size)\n","        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence;\n","        :                              hidden gives the hidden state and cell state for the last\n","        :                              element in the sequence \n","        '''\n","        \n","        \n","        embedded = self.embeddings(x_input)\n","        # embedded size: (seq_len, batch_size, embedding_size)\n","        if DEBUG_FLAG:\n","            print(f\"Encoder embedded size - {type(embedded)} - {embedded.shape}\")\n","#         embedded = embedded.view(1, 1, -1)\n","#         print(f\"Encoder embedded size - {type(embedded)} - {embedded.shape}\")\n","        \n","        lstm_out, self.hidden = self.lstm(embedded)\n","        if DEBUG_FLAG:\n","            print(f\"Encoder hidden_state size - {type(self.hidden)} - {self.hidden[0].shape}\")\n","        # lstm_out, self.hidden = self.lstm(x_input.view(x_input.shape[0], x_input.shape[1], self.input_size))\n","        \n","        return lstm_out, self.hidden     \n","    \n","    def init_hidden(self, batch_size):\n","        \n","        '''\n","        initialize hidden state\n","        : param batch_size:    x_input.shape[1]\n","        : return:              zeroed hidden state and cell state \n","        '''\n","        \n","        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n","                torch.zeros(self.num_layers, batch_size, self.hidden_size))"]},{"cell_type":"code","execution_count":8,"id":"1333309f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1333309f","executionInfo":{"status":"ok","timestamp":1683646952923,"user_tz":240,"elapsed":4,"user":{"displayName":"Aishwarya Malgonde","userId":"02733017902828562032"}},"outputId":"1c50fc4c-9f09-4273-8c90-3feb9dd35e5a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([43, 5, 60]), torch.Size([2, 5, 30]), torch.Size([2, 5, 30]))"]},"metadata":{},"execution_count":8}],"source":["enc = bilstm_encoder(vocab_size, 30, 20)\n","out, enc_hidden_state = enc.forward(X_train)\n","out.shape, enc_hidden_state[0].shape, enc_hidden_state[1].shape"]},{"cell_type":"code","execution_count":9,"id":"d18bead6","metadata":{"id":"d18bead6","executionInfo":{"status":"ok","timestamp":1683646952923,"user_tz":240,"elapsed":3,"user":{"displayName":"Aishwarya Malgonde","userId":"02733017902828562032"}}},"outputs":[],"source":["# No bi - (torch.Size([43, 5, 30]), torch.Size([1, 5, 30]), torch.Size([1, 5, 30]))\n","# With bi - (torch.Size([43, 5, 60]), torch.Size([2, 5, 30]), torch.Size([2, 5, 30]))"]},{"cell_type":"code","execution_count":10,"id":"4d108575","metadata":{"id":"4d108575","executionInfo":{"status":"ok","timestamp":1683646952924,"user_tz":240,"elapsed":4,"user":{"displayName":"Aishwarya Malgonde","userId":"02733017902828562032"}}},"outputs":[],"source":["class bilstm_decoder(nn.Module):\n","    ''' Decodes hidden state output by encoder '''\n","    \n","    def __init__(self, input_size, hidden_size, emb_size, output_size, num_layers = 2, dropout = 0):\n","\n","        '''\n","        : param input_size:     the number of features in the input X\n","        : param hidden_size:    the number of features in the hidden state h\n","        : param num_layers:     number of recurrent layers (i.e., 2 means there are\n","        :                       2 stacked LSTMs)\n","        '''\n","        \n","        super(bilstm_decoder, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        \n","        if DEBUG_FLAG:\n","            print(f\"Encoder: input_size {input_size} - hidden_size {hidden_size} - emb_size {emb_size} - output_size {output_size}\")\n","        \n","        # define embeddings\n","        self.embeddings = nn.Embedding(input_size, emb_size)\n","        \n","        self.lstm = nn.LSTM(input_size = emb_size,\n","                            hidden_size = hidden_size,\n","                            num_layers = num_layers,\n","                            bidirectional = False,\n","                            dropout = dropout)\n","        # bi is true\n","        # self.linear = nn.Linear(2*hidden_size, output_size)\n","        # num_layers = 1\n","        self.linear = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x_input, encoder_hidden_states):\n","        \n","        '''        \n","        : param x_input:                    should be 2D (1, batch_size) #, input_size)\n","        : param encoder_hidden_states:      hidden states\n","        : return output, hidden:            output gives all the hidden states in the sequence;\n","        :                                   hidden gives the hidden state and cell state for the last\n","        :                                   element in the sequence \n"," \n","        '''\n","        if DEBUG_FLAG:\n","            print(f\"Decoder x_input size - {x_input.shape}\")\n","        x_input = x_input.unsqueeze(0)\n","        # x_input size: (1, batch_size)\n","        if DEBUG_FLAG:\n","            print(f\"Decoder x_input size - {x_input.shape}\")\n","        \n","        embedded = self.embeddings(x_input)\n","        # embedded size: (1, batch_size, embedding_size)\n","        if DEBUG_FLAG:\n","            print(f\"Decoder embedded size - {embedded.shape}\")\n","            print(f\"Decoder encoder_hidden_states size - {encoder_hidden_states[0].shape}\")\n","            \n","        lstm_out, self.hidden = self.lstm(embedded, encoder_hidden_states)\n","        # lstm_out size: (1, batch_size, hidden_size)\n","        if DEBUG_FLAG:\n","            print(f\"Decoder lstm_out size - {lstm_out.shape}\")\n","            print(f\"Decoder hidden size - {self.hidden[0].shape}\")\n","        \n","        lstm_out = lstm_out.squeeze(0)\n","        if DEBUG_FLAG:\n","            print(f\"Decoder lstm_out size - {lstm_out.shape}\")\n","        output = self.linear(lstm_out) \n","        if DEBUG_FLAG:\n","            print(f\"Decoder output size - {output.shape}\")\n","        # output size: (1, batch_size, vocab_size)\n","        \n","#         output = output.squeeze(0)\n","#         print(f\"Decoder output size - {output.shape}\")\n","        # output size: (batch_size, vocab_size)\n","        \n","        return output, self.hidden"]},{"cell_type":"code","execution_count":11,"id":"723ed77a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"723ed77a","executionInfo":{"status":"ok","timestamp":1683646952924,"user_tz":240,"elapsed":4,"user":{"displayName":"Aishwarya Malgonde","userId":"02733017902828562032"}},"outputId":"190f6266-22c5-40fd-9628-3a65d6811e9a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([5, 5938]), torch.Size([2, 5, 30]), torch.Size([2, 5, 30]))"]},"metadata":{},"execution_count":11}],"source":["dec = bilstm_decoder(vocab_size, 30, 20, vocab_size)\n","out, hidden_state = dec.forward(Y_train[0], enc_hidden_state)\n","out.shape, hidden_state[0].shape, hidden_state[1].shape"]},{"cell_type":"code","execution_count":11,"id":"b39d0dd7","metadata":{"id":"b39d0dd7","executionInfo":{"status":"ok","timestamp":1683646953118,"user_tz":240,"elapsed":1,"user":{"displayName":"Aishwarya Malgonde","userId":"02733017902828562032"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"id":"5e705f38","metadata":{"id":"5e705f38","executionInfo":{"status":"ok","timestamp":1683646953244,"user_tz":240,"elapsed":3,"user":{"displayName":"Aishwarya Malgonde","userId":"02733017902828562032"}}},"outputs":[],"source":["class lstm_seq2seq(nn.Module):\n","    ''' train LSTM encoder-decoder and make predictions '''\n","    \n","    def __init__(self, encoder, decoder):\n","\n","        '''\n","        : param input_size:     the number of expected features in the input X\n","        : param hidden_size:    the number of features in the hidden state h\n","        '''\n","\n","        super(lstm_seq2seq, self).__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","\n","    def forward(self, source, target, target_vocab_size, teacher_force_ratio = 0.5):\n","        batch_size = source.shape[1]\n","        target_len = target.shape[0]\n","        \n","        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n","        \n","        # encoder outputs\n","        encoder_output, encoder_hidden = self.encoder.forward(source)\n","        \n","        # Grab start token\n","        x = target[0]\n","#         print(f\"seq2seq x size - {x.shape}\")\n","        \n","        for t in range(1, target_len):\n","            # decoder outputs\n","            decoder_output, decoder_hidden = self.decoder(x, encoder_hidden)\n","            \n","            outputs[t] = decoder_output\n","            # output size: (N, vocab_size)\n","            \n","            if DEBUG_FLAG: print(f\"seq2seq decoder_output size - {decoder_output.shape}\")\n","            best_guess = decoder_output.argmax(1)\n","            \n","            if DEBUG_FLAG: print(f\"seq2seq best_guess size - {best_guess.shape} - {best_guess}\")\n","            \n","            if DEBUG_FLAG: print(f\"seq2seq target size - {target[t].shape} - {target[t]}\")\n","            \n","            x = target[t] if random.random() < teacher_force_ratio else best_guess\n","            \n","        return outputs\n","    \n","    def predict(self, source, target_len, target_vocab_size):\n","        \n","        target_len = target_len+2\n","        batch_size = source.shape[1]\n","\n","        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n","\n","        # encoder outputs\n","        encoder_output, encoder_hidden = self.encoder.forward(source)\n","\n","        # Grab start token\n","        x = torch.from_numpy(np.array([sos_idx]*batch_size))\n","\n","        for t in range(1, target_len):\n","            # decoder outputs\n","            decoder_output, decoder_hidden = self.decoder(x, encoder_hidden)\n","\n","            outputs[t] = decoder_output\n","            # output size: (N, vocab_size)\n","\n","            best_guess = decoder_output.argmax(1)\n","            x = best_guess\n","#             outputs[t] = best_guess\n","\n","        return outputs"]},{"cell_type":"code","execution_count":36,"id":"cd08025a","metadata":{"id":"cd08025a","executionInfo":{"status":"ok","timestamp":1683647917913,"user_tz":240,"elapsed":124,"user":{"displayName":"Aishwarya Malgonde","userId":"02733017902828562032"}}},"outputs":[],"source":["def train(input_tensor, target_tensor, emb_size, hidden_size, vocab_size, load_model = False,\n","         num_epochs = 2, lr = 0.0005, batch_size = 5):\n","  \n","    sub_folder_name = f\"baseline_lr{lr}_bs{batch_size}_es{emb_size}_hs{hidden_size}\"\n","    models_directory = f\"models/{sub_folder_name}\"\n","\n","    if not os.path.isdir(models_directory):\n","      os.makedirs(models_directory)\n","    \n","    input_size_encoder = vocab_size             # german\n","    input_size_decoder = vocab_size             # english\n","    output_size = vocab_size                    # english\n","\n","    encoder_embedding_size = emb_size\n","    decoder_embedding_size = emb_size\n","\n","    # TENSORBOARD\n","    writer = SummaryWriter(f'tb/loss_plot/{sub_folder_name}')\n","    step = 0\n","\n","    encoder_net = bilstm_encoder(input_size_encoder, hidden_size, emb_size).to(device)\n","    decoder_net = bilstm_decoder(input_size_decoder, hidden_size, emb_size, output_size).to(device)\n","    \n","    model = lstm_seq2seq(encoder_net, decoder_net).to(device)\n","    \n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n","    \n","#     if load_model: load_checkpoint(torch.load('my_checkpoint.pth.ptar'), model, optimizer)\n","        \n","    # calculate number of batch iterations\n","    n_batches = int(input_tensor.shape[1] / batch_size)\n","    print(f\"Number of batches - {n_batches}\")\n","    \n","    # initialize array of losses \n","    losses = np.full(num_epochs, np.nan)\n","\n","    for epoch in range(1, num_epochs+1):\n","        \n","\n","    #         checkpoint = {'state_dict': model.state_dict(),\n","    #                       'optimizer': optimizer.state_dict()}\n","    #         save_checkpoint(checkpoint)\n","\n","        batch_loss = 0\n","        batch_count = 0\n","\n","        with trange(n_batches) as tr:\n","            for b in tr:\n","        \n","              # select data \n","              inp_data = input_tensor[:, b*batch_size : (b+1)*batch_size] #, :]\n","              target = target_tensor[:, b*batch_size : (b+1)*batch_size] #, :]\n","\n","              if torch.cuda.is_available():\n","                  inp_data, target = inp_data.cuda(), target.cuda()\n","              \n","  #             if step < 1:\n","  #                 print(f\"batch_size - {b*batch_size} - {(b+1)*batch_size}\")\n","  #                 print(f\"inp_data 0 - {inp_data}\")\n","\n","  #                 if step < 2:\n","  #                     print(f\"inp_data 0 - {inp_data.shape}\")\n","  #                     print(f\"target 0 - {target.shape}\")\n","\n","              output = model.forward(inp_data, target, output_size)\n","              # output shape: (target_len, batch_size, output_dim)\n","\n","  #             if step < 1:\n","  #                 print(f\"output size before reshape - {output.shape}\")\n","  #                 print(f\"target size before reshape - {target.shape}\")\n","\n","              output = output[1:].reshape(-1, output.shape[2])\n","              target = target[1:].reshape(-1)\n","\n","              if torch.cuda.is_available():\n","                  output = output.cuda()\n","\n","  #             if step < 1:\n","  #                 print(f\"output size after reshape - {output.shape}\")\n","  #                 print(f\"target size after reshape - {target.shape}\")\n","\n","  #                 output = output.argmax(2)\n","  #                 if step < 2:\n","  #                     print(f\"output 3 - {output.shape} - {type(output)} - {output[:5]}\")\n","  #                     print(f\"target 3 - {target.shape} - {type(target)} - {target[:5]}\")\n","\n","              # zero the gradient\n","              optimizer.zero_grad()\n","\n","              # compute the loss\n","              loss = criterion(output, target)\n","              batch_loss += loss.item()\n","\n","              # backpropagation\n","              loss.backward()\n","              torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1) # for healthy gradients\n","              optimizer.step()\n","\n","              writer.add_scalar('Training loss', loss, global_step=step)\n","              step += 1\n","\n","              batch_count += 1\n","              acc_batch_loss = batch_loss/batch_count\n","              # progress bar \n","              tr.set_postfix({\"epoch_num\":epoch, \"loss\":f\"{acc_batch_loss:.3f}\"})\n","                \n","        # loss for epoch \n","        batch_loss /= n_batches \n","        losses[epoch] = batch_loss\n","            \n","        # save models\n","        if (epoch > 4 and epoch % 2 == 0):\n","            torch.save(model, os.path.join(models_directory, f\"model_{epoch}\"))\n","        # break\n","\n","    with open(os.path.join(models_directory, \"loss.txt\"), 'w') as f:\n","      for s in loss:\n","        f.write(str(s) + '\\n')\n","\n","    torch.save(model, os.path.join(models_directory, f\"model_last_{epoch}\"))\n","            \n","    return losses, model"]},{"cell_type":"code","execution_count":37,"id":"c1fa70ea","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c1fa70ea","executionInfo":{"status":"ok","timestamp":1683652659212,"user_tz":240,"elapsed":4738891,"user":{"displayName":"Aishwarya Malgonde","userId":"02733017902828562032"}},"outputId":"baab219e-a7fb-47f1-dc1b-4b23057fe701"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of batches - 98\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 98/98 [00:47<00:00,  2.05it/s, epoch_num=1, loss=5.825]\n","100%|██████████| 98/98 [00:47<00:00,  2.06it/s, epoch_num=2, loss=4.635]\n","100%|██████████| 98/98 [00:47<00:00,  2.06it/s, epoch_num=3, loss=4.233]\n","100%|██████████| 98/98 [00:47<00:00,  2.06it/s, epoch_num=4, loss=3.951]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=5, loss=3.769]\n","100%|██████████| 98/98 [00:47<00:00,  2.05it/s, epoch_num=6, loss=3.634]\n","100%|██████████| 98/98 [00:47<00:00,  2.06it/s, epoch_num=7, loss=3.557]\n","100%|██████████| 98/98 [00:47<00:00,  2.06it/s, epoch_num=8, loss=3.488]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=9, loss=3.433]\n","100%|██████████| 98/98 [00:47<00:00,  2.06it/s, epoch_num=10, loss=3.347]\n","100%|██████████| 98/98 [00:47<00:00,  2.08it/s, epoch_num=11, loss=3.266]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=12, loss=3.195]\n","100%|██████████| 98/98 [00:47<00:00,  2.08it/s, epoch_num=13, loss=3.146]\n","100%|██████████| 98/98 [00:48<00:00,  2.04it/s, epoch_num=14, loss=3.110]\n","100%|██████████| 98/98 [00:48<00:00,  2.03it/s, epoch_num=15, loss=3.051]\n","100%|██████████| 98/98 [00:48<00:00,  2.03it/s, epoch_num=16, loss=2.969]\n","100%|██████████| 98/98 [00:48<00:00,  2.04it/s, epoch_num=17, loss=2.937]\n","100%|██████████| 98/98 [00:47<00:00,  2.05it/s, epoch_num=18, loss=2.899]\n","100%|██████████| 98/98 [00:48<00:00,  2.03it/s, epoch_num=19, loss=2.829]\n","100%|██████████| 98/98 [00:47<00:00,  2.04it/s, epoch_num=20, loss=2.799]\n","100%|██████████| 98/98 [00:47<00:00,  2.04it/s, epoch_num=21, loss=2.742]\n","100%|██████████| 98/98 [00:47<00:00,  2.06it/s, epoch_num=22, loss=2.708]\n","100%|██████████| 98/98 [00:47<00:00,  2.05it/s, epoch_num=23, loss=2.628]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=24, loss=2.569]\n","100%|██████████| 98/98 [00:47<00:00,  2.06it/s, epoch_num=25, loss=2.549]\n","100%|██████████| 98/98 [00:47<00:00,  2.06it/s, epoch_num=26, loss=2.480]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=27, loss=2.415]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=28, loss=2.355]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=29, loss=2.341]\n","100%|██████████| 98/98 [00:47<00:00,  2.05it/s, epoch_num=30, loss=2.258]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=31, loss=2.212]\n","100%|██████████| 98/98 [00:47<00:00,  2.06it/s, epoch_num=32, loss=2.129]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=33, loss=2.071]\n","100%|██████████| 98/98 [00:47<00:00,  2.05it/s, epoch_num=34, loss=2.021]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=35, loss=1.955]\n","100%|██████████| 98/98 [00:47<00:00,  2.06it/s, epoch_num=36, loss=1.874]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=37, loss=1.830]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=38, loss=1.787]\n","100%|██████████| 98/98 [00:46<00:00,  2.09it/s, epoch_num=39, loss=1.774]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=40, loss=1.728]\n","100%|██████████| 98/98 [00:46<00:00,  2.09it/s, epoch_num=41, loss=1.650]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=42, loss=1.644]\n","100%|██████████| 98/98 [00:47<00:00,  2.06it/s, epoch_num=43, loss=1.603]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=44, loss=1.600]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=45, loss=1.513]\n","100%|██████████| 98/98 [00:47<00:00,  2.08it/s, epoch_num=46, loss=1.501]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=47, loss=1.446]\n","100%|██████████| 98/98 [00:47<00:00,  2.08it/s, epoch_num=48, loss=1.461]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=49, loss=1.399]\n","100%|██████████| 98/98 [00:47<00:00,  2.08it/s, epoch_num=50, loss=1.362]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=51, loss=1.369]\n","100%|██████████| 98/98 [00:47<00:00,  2.08it/s, epoch_num=52, loss=1.338]\n","100%|██████████| 98/98 [00:47<00:00,  2.06it/s, epoch_num=53, loss=1.316]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=54, loss=1.305]\n","100%|██████████| 98/98 [00:47<00:00,  2.05it/s, epoch_num=55, loss=1.278]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=56, loss=1.272]\n","100%|██████████| 98/98 [00:47<00:00,  2.05it/s, epoch_num=57, loss=1.276]\n","100%|██████████| 98/98 [00:47<00:00,  2.05it/s, epoch_num=58, loss=1.250]\n","100%|██████████| 98/98 [00:47<00:00,  2.08it/s, epoch_num=59, loss=1.206]\n","100%|██████████| 98/98 [00:47<00:00,  2.06it/s, epoch_num=60, loss=1.217]\n","100%|██████████| 98/98 [00:47<00:00,  2.08it/s, epoch_num=61, loss=1.169]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=62, loss=1.180]\n","100%|██████████| 98/98 [00:46<00:00,  2.09it/s, epoch_num=63, loss=1.202]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=64, loss=1.165]\n","100%|██████████| 98/98 [00:46<00:00,  2.09it/s, epoch_num=65, loss=1.173]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=66, loss=1.162]\n","100%|██████████| 98/98 [00:46<00:00,  2.09it/s, epoch_num=67, loss=1.153]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=68, loss=1.145]\n","100%|██████████| 98/98 [00:46<00:00,  2.09it/s, epoch_num=69, loss=1.139]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=70, loss=1.126]\n","100%|██████████| 98/98 [00:46<00:00,  2.09it/s, epoch_num=71, loss=1.149]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=72, loss=1.118]\n","100%|██████████| 98/98 [00:47<00:00,  2.08it/s, epoch_num=73, loss=1.110]\n","100%|██████████| 98/98 [00:47<00:00,  2.08it/s, epoch_num=74, loss=1.106]\n","100%|██████████| 98/98 [00:47<00:00,  2.08it/s, epoch_num=75, loss=1.088]\n","100%|██████████| 98/98 [00:47<00:00,  2.08it/s, epoch_num=76, loss=1.084]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=77, loss=1.096]\n","100%|██████████| 98/98 [00:47<00:00,  2.08it/s, epoch_num=78, loss=1.082]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=79, loss=1.064]\n","100%|██████████| 98/98 [00:46<00:00,  2.09it/s, epoch_num=80, loss=1.057]\n","100%|██████████| 98/98 [00:47<00:00,  2.08it/s, epoch_num=81, loss=1.065]\n","100%|██████████| 98/98 [00:46<00:00,  2.09it/s, epoch_num=82, loss=1.094]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=83, loss=1.073]\n","100%|██████████| 98/98 [00:46<00:00,  2.09it/s, epoch_num=84, loss=1.052]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=85, loss=1.063]\n","100%|██████████| 98/98 [00:47<00:00,  2.08it/s, epoch_num=86, loss=1.029]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=87, loss=1.038]\n","100%|██████████| 98/98 [00:47<00:00,  2.09it/s, epoch_num=88, loss=1.035]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=89, loss=1.017]\n","100%|██████████| 98/98 [00:46<00:00,  2.09it/s, epoch_num=90, loss=1.050]\n","100%|██████████| 98/98 [00:47<00:00,  2.08it/s, epoch_num=91, loss=1.034]\n","100%|██████████| 98/98 [00:47<00:00,  2.08it/s, epoch_num=92, loss=1.025]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=93, loss=1.007]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=94, loss=1.028]\n","100%|██████████| 98/98 [00:46<00:00,  2.09it/s, epoch_num=95, loss=0.991]\n","100%|██████████| 98/98 [00:47<00:00,  2.07it/s, epoch_num=96, loss=0.984]\n","100%|██████████| 98/98 [00:47<00:00,  2.08it/s, epoch_num=97, loss=1.009]\n","100%|██████████| 98/98 [00:47<00:00,  2.08it/s, epoch_num=98, loss=0.975]\n","100%|██████████| 98/98 [00:46<00:00,  2.09it/s, epoch_num=99, loss=0.985]\n","100%|██████████| 98/98 [00:47<00:00,  2.08it/s, epoch_num=100, loss=0.966]\n"]}],"source":["loss, model = train(X_train, Y_train, emb_size=200, hidden_size=100, vocab_size=vocab_size,\n","            num_epochs = 100, lr = 0.001, batch_size = 64)"]},{"cell_type":"code","execution_count":40,"id":"bb8769fb","metadata":{"id":"bb8769fb","executionInfo":{"status":"ok","timestamp":1683652853853,"user_tz":240,"elapsed":139,"user":{"displayName":"Aishwarya Malgonde","userId":"02733017902828562032"}}},"outputs":[],"source":[]},{"cell_type":"markdown","id":"eb23767e","metadata":{"id":"eb23767e"},"source":["## Predict"]},{"cell_type":"code","execution_count":null,"id":"a394b72c","metadata":{"id":"a394b72c","outputId":"9b00eb8d-410b-4190-86fe-23c8f6ccc8da"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'models/model_1'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[80], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Model class must be defined somewhere\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model2 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/model_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model2\u001b[38;5;241m.\u001b[39meval()\n","File \u001b[0;32m~/Aishwarya/Learning/venv/lib/python3.11/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n","File \u001b[0;32m~/Aishwarya/Learning/venv/lib/python3.11/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n","File \u001b[0;32m~/Aishwarya/Learning/venv/lib/python3.11/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/model_1'"]}],"source":["# Model class must be defined somewhere\n","model2 = torch.load(\"models/model_1\")\n","model2.eval()"]},{"cell_type":"code","execution_count":null,"id":"057e9ca4","metadata":{"id":"057e9ca4","outputId":"452fe9c3-b7a5-41dd-df74-721ac5fe3f8c"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 79891.50it/s]"]},{"name":"stdout","output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# for tr in trange(10):\n","#     print(tr)\n","    \n","with trange(10) as tr:\n","    for it in tr:\n","        print(it)"]},{"cell_type":"code","execution_count":null,"id":"4e71e65a","metadata":{"id":"4e71e65a","outputId":"01689147-98d9-48ba-9a1f-ff1869b4bad0"},"outputs":[{"data":{"text/plain":["(torch.Size([43, 5]), torch.Size([127, 5, 4455]))"]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["outputs = model2.predict(X_val, max_decoder_len, vocab_size)\n","X_val.shape, outputs.shape"]},{"cell_type":"code","execution_count":null,"id":"ee0244ab","metadata":{"id":"ee0244ab"},"outputs":[],"source":["with open(os.path.join(target_folder, 'idx_to_vocab.json'), 'r') as fp:\n","    idx_to_vocab = json.load(fp)"]},{"cell_type":"code","execution_count":null,"id":"72d3654e","metadata":{"id":"72d3654e"},"outputs":[],"source":["idx_to_vocab['3643'] ,idx_to_vocab['3438']"]},{"cell_type":"code","execution_count":null,"id":"aa6f1d95","metadata":{"id":"aa6f1d95"},"outputs":[],"source":["Y_train[:,0], train_output[:,0]"]},{"cell_type":"code","execution_count":null,"id":"38517662","metadata":{"id":"38517662"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"da25446a","metadata":{"id":"da25446a"},"outputs":[],"source":["loss = nn.CrossEntropyLoss()\n","input = torch.randn(3, 5, requires_grad=True)\n","target = torch.empty(3, dtype=torch.long).random_(5)\n","output = loss(input, target)"]},{"cell_type":"code","execution_count":null,"id":"d146b8ff","metadata":{"id":"d146b8ff"},"outputs":[],"source":["input, target, input.shape, target.shape"]},{"cell_type":"code","execution_count":null,"id":"e67ebb16","metadata":{"id":"e67ebb16"},"outputs":[],"source":["type(model)"]},{"cell_type":"code","execution_count":null,"id":"b9ac96a2","metadata":{"id":"b9ac96a2","outputId":"a0a1ea80-17b0-4f0a-9f0e-e49131436c58"},"outputs":[{"data":{"text/plain":["(torch.Size([43, 5]), torch.Size([127, 5, 4455]))"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":null,"id":"fcdda92d","metadata":{"id":"fcdda92d","outputId":"64c4f6d1-72a9-4a5c-c646-be3fc1a057da"},"outputs":[{"data":{"text/plain":["(torch.Size([127, 5]),\n"," tensor([   0, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039,\n","         1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039,\n","         1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039,\n","         1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039,\n","         1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039,\n","         1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039,\n","         1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039,\n","         1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039,\n","         1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039,\n","         1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039,\n","         1039, 1039, 1039, 1039, 1039, 1039, 1039]))"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["outputs = outputs.argmax(2)\n","outputs.shape, outputs[:,0]"]},{"cell_type":"code","execution_count":null,"id":"418046c8","metadata":{"id":"418046c8"},"outputs":[],"source":["outputs"]},{"cell_type":"code","execution_count":null,"id":"e703d2e5","metadata":{"id":"e703d2e5","outputId":"acf58a4d-bf2b-4cdb-f1fa-cd302aa41b75"},"outputs":[{"name":"stdout","output_type":"stream","text":["Val encoder input - (43, 5)\n","Val decoder input - (127, 5)\n"]}],"source":["target_folder = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/LSTM_encoder_decoder/data/data_final_processed_v2\"\n","\n","## GET DATA\n","#sample data for checking network\n","fol1 = 'dev'\n","data_t = 'encode'\n","X_val_np = np.load(os.path.join(target_folder, fol1, f\"{fol1}_{data_t}.npy\"))\n","X_val_np = X_val_np[:5]\n","X_val_np = X_val_np.transpose(1,0)\n","# train_input = np.expand_dims(train_input, axis=-1) \n","print(f'Val encoder input - {X_val_np.shape}')\n","\n","\n","#sample data for checking network\n","fol1 = 'dev'\n","data_t = 'decode'\n","Y_val_np = np.load(os.path.join(target_folder, fol1, f\"{fol1}_{data_t}.npy\"))\n","Y_val_np = Y_val_np[:5]\n","Y_val_np = Y_val_np.transpose(1,0)\n","# train_output = np.expand_dims(train_output, axis=-1) \n","print(f'Val decoder input - {Y_val_np.shape}')\n"]},{"cell_type":"code","execution_count":null,"id":"6524cea9","metadata":{"id":"6524cea9","outputId":"fd5e811d-af61-4deb-ef47-407946a767d1"},"outputs":[{"data":{"text/plain":["(torch.Size([43, 5]), torch.Size([127, 5]))"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["# convert numpy array to tensors\n","X_val = torch.from_numpy(X_val_np).type(torch.int64) #torch.int64, torch.Tensor\n","Y_val = torch.from_numpy(Y_val_np).type(torch.int64)\n","\n","X_val.shape, Y_val.shape"]},{"cell_type":"code","execution_count":null,"id":"cd6d5882","metadata":{"id":"cd6d5882"},"outputs":[],"source":["max_decoder_len = data_info['max_decoder_len']"]},{"cell_type":"code","execution_count":null,"id":"46ab5fb8","metadata":{"id":"46ab5fb8"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"},"colab":{"provenance":[],"gpuType":"T4"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}