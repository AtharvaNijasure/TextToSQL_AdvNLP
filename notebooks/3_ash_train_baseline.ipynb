{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf3fefe7",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ce0231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, errno\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from utils import save_checkpoint, load_checkpoint\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ffed02",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbd13c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train encoder input - (43, 5)\n",
      "Train decoder input - (127, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 4455,\n",
       " 'max_encoder_len': 43,\n",
       " 'max_decoder_len': 125,\n",
       " 'pad_idx': 243,\n",
       " 'sos_idx': 244,\n",
       " 'eos_idx': 242}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_folder = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/LSTM_encoder_decoder/data/data_final_processed_v2\"\n",
    "\n",
    "## GET DATA\n",
    "#sample data for checking network\n",
    "fol1 = 'train'\n",
    "data_t = 'encode'\n",
    "X_train_np = np.load(os.path.join(target_folder, fol1, f\"{fol1}_{data_t}.npy\"))\n",
    "X_train_np = X_train_np[:5]\n",
    "X_train_np = X_train_np.transpose(1,0)\n",
    "# train_input = np.expand_dims(train_input, axis=-1) \n",
    "print(f'Train encoder input - {X_train_np.shape}')\n",
    "\n",
    "\n",
    "#sample data for checking network\n",
    "fol1 = 'train'\n",
    "data_t = 'decode'\n",
    "Y_train_np = np.load(os.path.join(target_folder, fol1, f\"{fol1}_{data_t}.npy\"))\n",
    "Y_train_np = Y_train_np[:5]\n",
    "Y_train_np = Y_train_np.transpose(1,0)\n",
    "# train_output = np.expand_dims(train_output, axis=-1) \n",
    "print(f'Train decoder input - {Y_train_np.shape}')\n",
    "\n",
    "with open(os.path.join(target_folder, 'data_info.json'), 'r') as fp:\n",
    "    data_info = json.load(fp)\n",
    "    \n",
    "pad_idx = data_info['pad_idx']\n",
    "sos_idx = data_info['sos_idx']\n",
    "vocab_size = data_info['vocab_size']\n",
    "    \n",
    "data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d0dd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([43, 5]), torch.Size([127, 5]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert numpy array to tensors\n",
    "X_train = torch.from_numpy(X_train_np).type(torch.int64) #torch.int64, torch.Tensor\n",
    "Y_train = torch.from_numpy(Y_train_np).type(torch.int64)\n",
    "\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6ab015",
   "metadata": {},
   "source": [
    "## Build Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24c30349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR PRINTING INTERMEDIATE TORCH SIZES\n",
    "DEBUG_FLAG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "205cf51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bilstm_encoder(nn.Module):\n",
    "    ''' Encodes time-series sequence '''\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, emb_size, num_layers = 1, dropout = 0):\n",
    "        \n",
    "        '''\n",
    "        : param input_size:     the number of features in the input X, eg: word embeddings\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        : param num_layers:     number of recurrent layers (i.e., 2 means there are\n",
    "        :                       2 stacked LSTMs)\n",
    "        '''\n",
    "        \n",
    "        super(bilstm_encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Encoder: input_size {input_size} - hidden_size {hidden_size} - emb_size {emb_size}\")\n",
    "\n",
    "        # define embeddings\n",
    "        self.embeddings = nn.Embedding(input_size, emb_size)\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size = emb_size,\n",
    "                            hidden_size = hidden_size,\n",
    "                            num_layers = num_layers,\n",
    "                            bidirectional = True,\n",
    "                            dropout = dropout)\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        \n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch) #, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence;\n",
    "        :                              hidden gives the hidden state and cell state for the last\n",
    "        :                              element in the sequence \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        embedded = self.embeddings(x_input)\n",
    "        # embedded size: (seq_len, batch_size, embedding_size)\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Encoder embedded size - {type(embedded)} - {embedded.shape}\")\n",
    "#         embedded = embedded.view(1, 1, -1)\n",
    "#         print(f\"Encoder embedded size - {type(embedded)} - {embedded.shape}\")\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(embedded)\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Encoder hidden_state size - {type(self.hidden)} - {self.hidden[0].shape}\")\n",
    "        # lstm_out, self.hidden = self.lstm(x_input.view(x_input.shape[0], x_input.shape[1], self.input_size))\n",
    "        \n",
    "        return lstm_out, self.hidden     \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        '''\n",
    "        initialize hidden state\n",
    "        : param batch_size:    x_input.shape[1]\n",
    "        : return:              zeroed hidden state and cell state \n",
    "        '''\n",
    "        \n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1333309f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([43, 5, 60]), torch.Size([2, 5, 30]), torch.Size([2, 5, 30]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = bilstm_encoder(vocab_size, 30, 20)\n",
    "out, enc_hidden_state = enc.forward(X_train)\n",
    "out.shape, enc_hidden_state[0].shape, enc_hidden_state[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d18bead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No bi - (torch.Size([43, 5, 30]), torch.Size([1, 5, 30]), torch.Size([1, 5, 30]))\n",
    "# With bi - (torch.Size([43, 5, 60]), torch.Size([2, 5, 30]), torch.Size([2, 5, 30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d108575",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bilstm_decoder(nn.Module):\n",
    "    ''' Decodes hidden state output by encoder '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, emb_size, output_size, num_layers = 1, dropout = 0):\n",
    "\n",
    "        '''\n",
    "        : param input_size:     the number of features in the input X\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        : param num_layers:     number of recurrent layers (i.e., 2 means there are\n",
    "        :                       2 stacked LSTMs)\n",
    "        '''\n",
    "        \n",
    "        super(bilstm_decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Encoder: input_size {input_size} - hidden_size {hidden_size} - emb_size {emb_size} - output_size {output_size}\")\n",
    "        \n",
    "        # define embeddings\n",
    "        self.embeddings = nn.Embedding(input_size, emb_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = emb_size,\n",
    "                            hidden_size = hidden_size,\n",
    "                            num_layers = num_layers,\n",
    "                            bidirectional = True,\n",
    "                            dropout = dropout)\n",
    "        \n",
    "        self.linear = nn.Linear(2*hidden_size, output_size)           \n",
    "\n",
    "    def forward(self, x_input, encoder_hidden_states):\n",
    "        \n",
    "        '''        \n",
    "        : param x_input:                    should be 2D (1, batch_size) #, input_size)\n",
    "        : param encoder_hidden_states:      hidden states\n",
    "        : return output, hidden:            output gives all the hidden states in the sequence;\n",
    "        :                                   hidden gives the hidden state and cell state for the last\n",
    "        :                                   element in the sequence \n",
    " \n",
    "        '''\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Decoder x_input size - {x_input.shape}\")\n",
    "        x_input = x_input.unsqueeze(0)\n",
    "        # x_input size: (1, batch_size)\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Decoder x_input size - {x_input.shape}\")\n",
    "        \n",
    "        embedded = self.embeddings(x_input)\n",
    "        # embedded size: (1, batch_size, embedding_size)\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Decoder embedded size - {embedded.shape}\")\n",
    "        \n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Decoder encoder_hidden_states size - {encoder_hidden_states[0].shape}\")\n",
    "        lstm_out, self.hidden = self.lstm(embedded, encoder_hidden_states)\n",
    "        # lstm_out size: (1, batch_size, hidden_size)\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Decoder lstm_out size - {lstm_out.shape}\")\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Decoder hidden size - {self.hidden[0].shape}\")\n",
    "        \n",
    "        lstm_out = lstm_out.squeeze(0)\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Decoder lstm_out size - {lstm_out.shape}\")\n",
    "        output = self.linear(lstm_out) \n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Decoder output size - {output.shape}\")\n",
    "        # output size: (1, batch_size, vocab_size)\n",
    "        \n",
    "#         output = output.squeeze(0)\n",
    "#         print(f\"Decoder output size - {output.shape}\")\n",
    "        # output size: (batch_size, vocab_size)\n",
    "        \n",
    "        return output, self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "723ed77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4455]), torch.Size([2, 5, 30]), torch.Size([2, 5, 30]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = bilstm_decoder(vocab_size, 30, 20, vocab_size)\n",
    "out, hidden_state = dec.forward(Y_train[0], enc_hidden_state)\n",
    "out.shape, hidden_state[0].shape, hidden_state[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39d0dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e705f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_seq2seq(nn.Module):\n",
    "    ''' train LSTM encoder-decoder and make predictions '''\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "\n",
    "        '''\n",
    "        : param input_size:     the number of expected features in the input X\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        '''\n",
    "\n",
    "        super(lstm_seq2seq, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "\n",
    "    def forward(self, source, target, target_vocab_size, teacher_force_ratio = 0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        \n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "        \n",
    "        # encoder outputs\n",
    "        encoder_output, encoder_hidden = self.encoder.forward(source)\n",
    "        \n",
    "        # Grab start token\n",
    "        x = target[0]\n",
    "#         print(f\"seq2seq x size - {x.shape}\")\n",
    "        \n",
    "        for t in range(1, target_len):\n",
    "            # decoder outputs\n",
    "            decoder_output, decoder_hidden = self.decoder(x, encoder_hidden)\n",
    "            \n",
    "            outputs[t] = decoder_output\n",
    "            # output size: (N, vocab_size)\n",
    "            \n",
    "            if DEBUG_FLAG: print(f\"seq2seq decoder_output size - {decoder_output.shape}\")\n",
    "            best_guess = decoder_output.argmax(1)\n",
    "            \n",
    "            if DEBUG_FLAG: print(f\"seq2seq best_guess size - {best_guess.shape} - {best_guess}\")\n",
    "            \n",
    "            if DEBUG_FLAG: print(f\"seq2seq target size - {target[t].shape} - {target[t]}\")\n",
    "            \n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "            \n",
    "        return outputs\n",
    "    \n",
    "    def predict(self, source, target_len, target_vocab_size):\n",
    "        \n",
    "        target_len = target_len+2\n",
    "        batch_size = source.shape[1]\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "\n",
    "        # encoder outputs\n",
    "        encoder_output, encoder_hidden = self.encoder.forward(source)\n",
    "\n",
    "        # Grab start token\n",
    "        x = torch.from_numpy(np.array([sos_idx]*batch_size))\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # decoder outputs\n",
    "            decoder_output, decoder_hidden = self.decoder(x, encoder_hidden)\n",
    "\n",
    "            outputs[t] = decoder_output\n",
    "            # output size: (N, vocab_size)\n",
    "\n",
    "            best_guess = decoder_output.argmax(1)\n",
    "            x = best_guess\n",
    "#             outputs[t] = best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd08025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, emb_size, hidden_size, vocab_size, load_model = False,\n",
    "         num_epochs = 2, lr = 0.001, batch_size = 5):\n",
    "    \n",
    "    input_size_encoder = vocab_size             # german\n",
    "    input_size_decoder = vocab_size             # english\n",
    "    output_size = vocab_size                    # english\n",
    "\n",
    "    encoder_embedding_size = emb_size\n",
    "    decoder_embedding_size = emb_size\n",
    "\n",
    "    # TENSORBOARD\n",
    "    writer = SummaryWriter(f'runs/loss_plot')\n",
    "    step = 0\n",
    "\n",
    "    encoder_net = bilstm_encoder(input_size_encoder, hidden_size, emb_size).to(device)\n",
    "    decoder_net = bilstm_decoder(input_size_decoder, hidden_size, emb_size, output_size).to(device)\n",
    "    \n",
    "    model = lstm_seq2seq(encoder_net, decoder_net).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "    \n",
    "#     if load_model: load_checkpoint(torch.load('my_checkpoint.pth.ptar'), model, optimizer)\n",
    "        \n",
    "    # calculate number of batch iterations\n",
    "    n_batches = int(input_tensor.shape[1] / batch_size)\n",
    "    print(f\"Number of batches - {n_batches}\")\n",
    "    \n",
    "    # initialize array of losses \n",
    "    losses = np.full(num_epochs, np.nan)\n",
    "\n",
    "#     with trange(num_epochs) as tr:\n",
    "#         for it in tr:\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "\n",
    "    #         checkpoint = {'state_dict': model.state_dict(),\n",
    "    #                       'optimizer': optimizer.state_dict()}\n",
    "    #         save_checkpoint(checkpoint)\n",
    "\n",
    "        batch_loss = 0\n",
    "\n",
    "        for b in range(n_batches):\n",
    "            for it in tr:\n",
    "                # select data \n",
    "                inp_data = input_tensor[:, b: b + batch_size] #, :]\n",
    "                target = target_tensor[:, b: b + batch_size] #, :]\n",
    "                \n",
    "#                 if step < 2:\n",
    "#                     print(f\"inp_data 0 - {inp_data.shape}\")\n",
    "#                     print(f\"target 0 - {target.shape}\")\n",
    "                \n",
    "                output = model.forward(inp_data, target, output_size)\n",
    "                # output shape: (target_len, batch_size, output_dim)\n",
    "                \n",
    "#                 if step < 1:\n",
    "#                     print(f\"output size before reshape - {output.shape}\")\n",
    "#                     print(f\"target size before reshape - {target.shape}\")\n",
    "                \n",
    "                output = output[1:].reshape(-1, output.shape[2])\n",
    "                target = target[1:].reshape(-1)\n",
    "                \n",
    "#                 if step < 1:\n",
    "#                     print(f\"output size after reshape - {output.shape}\")\n",
    "#                     print(f\"target size after reshape - {target.shape}\")\n",
    "                    \n",
    "#                 output = output.argmax(2)\n",
    "#                 if step < 2:\n",
    "#                     print(f\"output 3 - {output.shape} - {type(output)} - {output[:5]}\")\n",
    "#                     print(f\"target 3 - {target.shape} - {type(target)} - {target[:5]}\")\n",
    "                \n",
    "                # zero the gradient\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # compute the loss\n",
    "                loss = criterion(output, target)\n",
    "                batch_loss += loss.item()\n",
    "                \n",
    "                # backpropagation\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1) # for healthy gradients\n",
    "                optimizer.step()\n",
    "\n",
    "                writer.add_scalar('Training loss', loss, global_step=step)\n",
    "                step += 1\n",
    "                \n",
    "                # progress bar \n",
    "                tr.set_postfix(loss=\"{0:.3f}\".format(batch_loss))\n",
    "                \n",
    "            # loss for epoch \n",
    "            batch_loss /= n_batches \n",
    "            losses[it] = batch_loss\n",
    "            \n",
    "#             # progress bar \n",
    "#             tr.set_postfix(loss=\"{0:.3f}\".format(batch_loss))\n",
    "            \n",
    "    return losses, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1fa70ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss, model \u001b[38;5;241m=\u001b[39m train(\u001b[43mX_train\u001b[49m, Y_train, emb_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, vocab_size\u001b[38;5;241m=\u001b[39mvocab_size,\n\u001b[1;32m      2\u001b[0m             num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "loss, model = train(X_train, Y_train, emb_size=20, hidden_size=10, vocab_size=vocab_size,\n",
    "            num_epochs = 10, lr = 0.001, batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bb8769fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9eec09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb23767e",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a394b72c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/model_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Model class must be defined somewhere\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model2 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/model_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model2\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/Aishwarya/Learning/venv/lib/python3.11/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/Aishwarya/Learning/venv/lib/python3.11/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/Aishwarya/Learning/venv/lib/python3.11/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/model_1'"
     ]
    }
   ],
   "source": [
    "# Model class must be defined somewhere\n",
    "model2 = torch.load(\"models/model_1\")\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4e71e65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([43, 5]), torch.Size([127, 5, 4455]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model2.predict(X_val, max_decoder_len, vocab_size)\n",
    "X_val.shape, outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee0244ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(target_folder, 'idx_to_vocab.json'), 'r') as fp:\n",
    "    idx_to_vocab = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d3654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_vocab['3643'] ,idx_to_vocab['3438']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6f1d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train[:,0], train_output[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38517662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da25446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d146b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "input, target, input.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ebb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b9ac96a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([43, 5]), torch.Size([127, 5, 4455]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fcdda92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([127, 5]),\n",
       " tensor([   0, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039,\n",
       "         1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039,\n",
       "         1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039,\n",
       "         1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039,\n",
       "         1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039,\n",
       "         1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039,\n",
       "         1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039,\n",
       "         1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039,\n",
       "         1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039,\n",
       "         1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039,\n",
       "         1039, 1039, 1039, 1039, 1039, 1039, 1039]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = outputs.argmax(2)\n",
    "outputs.shape, outputs[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418046c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e703d2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val encoder input - (43, 5)\n",
      "Val decoder input - (127, 5)\n"
     ]
    }
   ],
   "source": [
    "target_folder = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/LSTM_encoder_decoder/data/data_final_processed_v2\"\n",
    "\n",
    "## GET DATA\n",
    "#sample data for checking network\n",
    "fol1 = 'dev'\n",
    "data_t = 'encode'\n",
    "X_val_np = np.load(os.path.join(target_folder, fol1, f\"{fol1}_{data_t}.npy\"))\n",
    "X_val_np = X_val_np[:5]\n",
    "X_val_np = X_val_np.transpose(1,0)\n",
    "# train_input = np.expand_dims(train_input, axis=-1) \n",
    "print(f'Val encoder input - {X_val_np.shape}')\n",
    "\n",
    "\n",
    "#sample data for checking network\n",
    "fol1 = 'dev'\n",
    "data_t = 'decode'\n",
    "Y_val_np = np.load(os.path.join(target_folder, fol1, f\"{fol1}_{data_t}.npy\"))\n",
    "Y_val_np = Y_val_np[:5]\n",
    "Y_val_np = Y_val_np.transpose(1,0)\n",
    "# train_output = np.expand_dims(train_output, axis=-1) \n",
    "print(f'Val decoder input - {Y_val_np.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6524cea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([43, 5]), torch.Size([127, 5]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert numpy array to tensors\n",
    "X_val = torch.from_numpy(X_val_np).type(torch.int64) #torch.int64, torch.Tensor\n",
    "Y_val = torch.from_numpy(Y_val_np).type(torch.int64)\n",
    "\n",
    "X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd6d5882",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_decoder_len = data_info['max_decoder_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab5fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
