{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc49d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22434,
     "status": "ok",
     "timestamp": 1683756039554,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "44dc49d1",
    "outputId": "f71e642f-cc11-4720-847d-9983cfafdaad"
   },
   "outputs": [],
   "source": [
    "# # MOUNTING GOOGLE DRIVE\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# import os\n",
    "# print(os.getcwd())\n",
    "\n",
    "# wd = '/content/drive/MyDrive/CS 685/cs685_project/notebooks'\n",
    "# print(os.listdir(wd))\n",
    "# os.chdir(wd)\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EOrjkzvAqCng",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28261,
     "status": "ok",
     "timestamp": 1683756067809,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "EOrjkzvAqCng",
    "outputId": "a12095b7-7146-43c2-e55b-2153180dfe64",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tokenizers\n",
    "# !pip install transformers\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee9ce01e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13533,
     "status": "ok",
     "timestamp": 1683756081335,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "ee9ce01e",
    "outputId": "754fbbbc-1c71-4490-81a9-e08292caace6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "from load_dataset import Text2SQLDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tokenizers import AddedToken\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "from transformers import BertModel, T5ForConditionalGeneration, T5Tokenizer, BertTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc843bcd",
   "metadata": {
    "id": "bc843bcd"
   },
   "source": [
    "## Need to replace BERT as GNN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17abad9d",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1683756081335,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "17abad9d"
   },
   "outputs": [],
   "source": [
    "# FOR PRINTING INTERMEDIATE TORCH SIZES\n",
    "DEBUG_FLAG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4921e160",
   "metadata": {
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1683756383738,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "4921e160"
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, bert_hidden_size, t5_hidden_size, max_input_length, \n",
    "                 max_output_length, bert_model, t5_model, batch_size):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        \n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "        self.t5_hidden_size = t5_hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        self.t5 = T5ForConditionalGeneration.from_pretrained(t5_model)\n",
    "#         self.linear = nn.Linear(bert_hidden_size, t5_hidden_size)\n",
    "        self.linear = nn.Linear(bert_hidden_size, max_input_length*t5_hidden_size)\n",
    "    \n",
    "        self.t5.config.is_encoder_decoder = False\n",
    "    \n",
    "\n",
    "    def forward(self, input_ids, input_mask,\n",
    "                decoder_input_ids, decoder_attention_mask):\n",
    "        \n",
    "        # Encode input with BERT\n",
    "        _, bert_output = self.bert(input_ids=input_ids, attention_mask=input_mask, return_dict=False)\n",
    "        \n",
    "        if DEBUG_FLAG: print(f\"bert_output - {bert_output.size()}\")\n",
    "        \n",
    "        # Transform BERT output to T5 input shape\n",
    "        t5_input = self.linear(bert_output)\n",
    "        if DEBUG_FLAG: print(f\"bert_output linear - {t5_input.size()}\")\n",
    "        \n",
    "#         t5_input = t5_input.unsqueeze(1).repeat(1, self.max_output_length, 1)\n",
    "#         if DEBUG_FLAG: print(f\"bert_output linear unsqueeze - {t5_input.size()}\")\n",
    "        \n",
    "        t5_input = t5_input.view(self.batch_size, self.max_input_length, self.t5_hidden_size)\n",
    "        t5_input = t5_input.unsqueeze(0)\n",
    "        if DEBUG_FLAG: print(f\"t5_input - {t5_input.size()}\")\n",
    "        \n",
    "#         t5_outputs = self.t5(decoder_input_ids=decoder_input_ids,\n",
    "#                              decoder_attention_mask=decoder_attention_mask,\n",
    "#                              encoder_outputs=t5_input\n",
    "#                             )\n",
    "#         if DEBUG_FLAG: print(f\"t5_input logits - {(t5_outputs.logits).size()}\")\n",
    "#         return t5_outputs.logits\n",
    "    \n",
    "        t5_outputs = self.t5(labels=decoder_input_ids,\n",
    "                             decoder_attention_mask=decoder_attention_mask,\n",
    "                             encoder_outputs=t5_input,\n",
    "                             return_dict = True\n",
    "                            )\n",
    "        \n",
    "        if DEBUG_FLAG: print(f\"t5_input - {type(t5_outputs)}\")\n",
    "        return t5_outputs\n",
    "\n",
    "    def predict(self, input_ids, input_mask, batch_size, t5_tokenizer):\n",
    "        \n",
    "        self.bert.eval()\n",
    "        _, bert_output = self.bert(input_ids=input_ids, attention_mask=input_mask, return_dict=False)\n",
    "        if DEBUG_FLAG: print(f\"bert_output - {bert_output.size()}\")\n",
    "        \n",
    "        # Transform BERT output to T5 input shape\n",
    "        t5_input = self.linear(bert_output)\n",
    "        if DEBUG_FLAG: print(f\"t5_input - {t5_input.size()} - {t5_input}\")\n",
    "        t5_input = t5_input.view(batch_size, self.max_input_length, self.t5_hidden_size)\n",
    "        t5_input = t5_input.unsqueeze(0)\n",
    "        if DEBUG_FLAG: print(f\"t5_input - {t5_input.size()} - {t5_input}\")\n",
    "        \n",
    "        # Generate initial input for T5 decoder\n",
    "        start_token = t5_tokenizer.pad_token_id\n",
    "        \n",
    "#         decoder_input_ids = torch.tensor([start_token] * batch_size).unsqueeze(0)\n",
    "#         decoder_attention_mask = torch.tensor([1] * batch_size).unsqueeze(0)\n",
    "        \n",
    "        decoder_input_ids = torch.tensor([start_token]).unsqueeze(0)\n",
    "        decoder_attention_mask = torch.tensor([1]).unsqueeze(0)\n",
    "    \n",
    "#         decoder_input_ids = decoder_input_ids.view(decoder_input_ids.shape[1],\n",
    "#                                                    decoder_input_ids.shape[0])\n",
    "#         decoder_attention_mask = decoder_attention_mask.view(decoder_attention_mask.shape[1],\n",
    "#                                                              decoder_attention_mask.shape[0])\n",
    "        \n",
    "        print(f\"decoder_input_ids - {decoder_input_ids.size()}\")\n",
    "        print(f\"decoder_attention_mask - {decoder_attention_mask.size()}\")\n",
    "        \n",
    "        print(f\"initial decoder_input_ids - {decoder_input_ids}\")\n",
    "        \n",
    "        # Use the model to get output logits\n",
    "        # Predict the output\n",
    "        self.t5.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(50):  # Maximum length of generated sequence\n",
    "                t5_outputs = self.t5(decoder_input_ids=decoder_input_ids,\n",
    "                                     decoder_attention_mask=decoder_attention_mask,\n",
    "                                     encoder_outputs=t5_input)\n",
    "#                 print(f\"t5_outputs - {t5_outputs}\")\n",
    "                print(f\"t5_outputs logits - {(t5_outputs.logits).size()}\")\n",
    "    \n",
    "                next_token_logits = t5_outputs.logits[:, -1, :]\n",
    "                print(f\"next_token_logits - {next_token_logits.size()}\")\n",
    "            \n",
    "#                 next_token_id = torch.argmax(next_token_logits, dim=-1)\n",
    "                next_token_id = next_token_logits.argmax(1)\n",
    "#                 print(f\"next_token_id - {next_token_id.size()}\")\n",
    "#                 print(f\"next_token_id.unsqueeze(-1) - {next_token_id.unsqueeze(-1).size()}\")\n",
    "                decoder_input_ids = torch.cat([decoder_input_ids, next_token_id.unsqueeze(-1)], dim=-1)\n",
    "                decoder_attention_mask = torch.cat([decoder_attention_mask,\n",
    "                                                    torch.ones_like(next_token_id.unsqueeze(-1))], dim=-1)\n",
    "\n",
    "                if next_token_id == t5_tokenizer.eos_token_id:\n",
    "                    break\n",
    "                \n",
    "                print(f\"pred decoder_input_ids - {decoder_input_ids}\")\n",
    "                \n",
    "#                 break\n",
    "        \n",
    "        # generated_text\n",
    "        t5_outputs = t5_tokenizer.decode(decoder_input_ids.squeeze(), skip_special_tokens=True)\n",
    "        \n",
    "        return t5_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e519d2d4",
   "metadata": {
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1683756664817,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "e519d2d4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(train_filepath, batch_size, bert_hidden_size, t5_hidden_size, lr, num_epochs,\n",
    "         max_input_length, max_output_length, bert_model, t5_model):\n",
    "    \n",
    "    sub_folder_name = f\"BERT_T5_lr{lr}_bs{batch_size}_{bert_model}_{t5_model}\"\n",
    "    models_directory = f\"models/{sub_folder_name}\"\n",
    "\n",
    "    if not os.path.isdir(models_directory):\n",
    "        os.makedirs(models_directory)\n",
    "        \n",
    "    # TENSORBOARD\n",
    "    writer = SummaryWriter(f'tb/loss_plot/{sub_folder_name}')\n",
    "    \n",
    "    train_dataset = Text2SQLDataset(\n",
    "            dir_ = train_filepath,\n",
    "            mode = \"train\")\n",
    "\n",
    "    train_dataloder = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size = batch_size, \n",
    "            shuffle = True,\n",
    "            collate_fn = lambda x: x,\n",
    "            drop_last = True\n",
    "        )\n",
    "    \n",
    "    print(f\"Number of batches - {len(train_dataloder)}\")\n",
    "\n",
    "    # Define BERT and T5 tokenizers\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "    t5_tokenizer = T5Tokenizer.from_pretrained(t5_model)\n",
    "    print(f\"Tokenizers loaded\")\n",
    "\n",
    "    model = EncoderDecoder(bert_hidden_size, t5_hidden_size, \n",
    "                           max_input_length, max_output_length,\n",
    "                           bert_model, t5_model, batch_size).to(device)\n",
    "    print(f\"Model loaded\")\n",
    "#     print(f\"{model.config.decoder_start_token_id}\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    print(f\"Otimizer - Adam\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=t5_tokenizer.pad_token_id)\n",
    "    print(f\"CrossEntropyLoss initialized\")\n",
    "    \n",
    "    # initialize array of losses \n",
    "    losses = {'train': {}, \"val\": {}}\n",
    "\n",
    "    # for epoch in range(num_epochs):\n",
    "    with trange(num_epochs) as tr:\n",
    "        for epoch in tr:\n",
    "            \n",
    "            # Train the model\n",
    "            model.train()\n",
    "            \n",
    "            batch_loss = 0\n",
    "            \n",
    "            for idx, batch in enumerate(train_dataloder):\n",
    "\n",
    "                batch_inputs = [data[0] for data in batch]\n",
    "                batch_sqls = [data[1] for data in batch]\n",
    "\n",
    "                if DEBUG_FLAG:\n",
    "                    if epoch == 0 and idx == 0:\n",
    "                        print(f\"batch_inputs - {type(batch_inputs)} {len(batch_inputs)}\")\n",
    "                        print(f\"batch_sqls - {type(batch_sqls)} {len(batch_sqls)}\")\n",
    "                        \n",
    "#                 for temp_i, temp in enumerate(batch_inputs):\n",
    "#                     print(f\"batch_inputs - {batch_inputs[temp_i]}\")\n",
    "#                     print(f\"batch_sqls - {batch_sqls[temp_i]}\")\n",
    "\n",
    "                tokenized_inputs = bert_tokenizer(batch_inputs,\n",
    "                                                  add_special_tokens=True,\n",
    "                                                  padding=\"max_length\", #True,\n",
    "                                                  max_length=max_input_length,\n",
    "                                                  #pad_to_max_length=True,\n",
    "                                                  return_tensors='pt',\n",
    "                                                  truncation=True)\n",
    "\n",
    "                encoder_input_ids = tokenized_inputs[\"input_ids\"].to(device)\n",
    "                encoder_input_attention_mask = tokenized_inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "#                 print(f\"encoder_input_ids - {encoder_input_ids}\")\n",
    "                tokenized_outputs = t5_tokenizer(batch_sqls,\n",
    "                                                 add_special_tokens=True,\n",
    "                                                 padding=\"max_length\", #True,\n",
    "                                                 max_length=max_output_length,\n",
    "                                                 #pad_to_max_length=True,\n",
    "                                                 return_tensors='pt',\n",
    "                                                 truncation=True)\n",
    "\n",
    "\n",
    "                decoder_input_ids = tokenized_outputs[\"input_ids\"].to(device)\n",
    "                # replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "                decoder_input_ids[decoder_input_ids == t5_tokenizer.pad_token_id] = -100\n",
    "                decoder_attention_mask = tokenized_outputs[\"attention_mask\"].to(device)\n",
    "#                 labels = None #tokenized_outputs[\"attention_mask\"].to(device)\n",
    "\n",
    "#                 print(f\"decoder_input_ids - {decoder_input_ids}\")\n",
    "\n",
    "                if DEBUG_FLAG and epoch == 0 and idx == 0:\n",
    "                    print(f\"encoder_input_ids - {encoder_input_ids.size()}\")\n",
    "                    print(f\"encoder_input_attention_mask - {encoder_input_attention_mask.size()}\")\n",
    "                    print(f\"decoder_input_ids - {decoder_input_ids.size()}\")\n",
    "                    print(f\"decoder_attention_mask - {decoder_attention_mask.size()}\")\n",
    "\n",
    "                # Clear gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                model_output = model(encoder_input_ids,\n",
    "                               encoder_input_attention_mask,\n",
    "                               decoder_input_ids,\n",
    "                               decoder_attention_mask)\n",
    "#                                labels=labels)\n",
    "                \n",
    "                output = model_output[\"logits\"]\n",
    "#                 print(f\"output - {output.size()}\")\n",
    "#                 print(f\"decoder_input_ids - {decoder_input_ids.size()}\")\n",
    "                \n",
    "                output_resize = output.view(output.shape[0]*output.shape[1], output.shape[2])\n",
    "                decoder_input_ids_resize = decoder_input_ids.view(decoder_input_ids.shape[0]*decoder_input_ids.shape[1])\n",
    "                \n",
    "#                 print(f\"output_resize - {output_resize.size()}\")\n",
    "#                 print(f\"decoder_input_ids_resize - {decoder_input_ids_resize.size()}\")\n",
    "                    \n",
    "#                 loss = criterion(output_resize, decoder_input_ids_resize)\n",
    "#                 batch_loss += loss.item()\n",
    "                \n",
    "#                 print(f\"output - {model_output}\")\n",
    "                loss = model_output[\"loss\"]\n",
    "                batch_loss += loss\n",
    "                \n",
    "#                 predicted_classes = torch.argmax(output_resize, dim=-1)\n",
    "                \n",
    "#                 print(f\"output_resize - {predicted_classes.size} - {predicted_classes}\")\n",
    "#                 print(f\"decoder_input_ids_resize - {decoder_input_ids_resize}\")\n",
    "\n",
    "                # backpropagation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                break\n",
    "                \n",
    "            batch_loss /= len(train_dataloder) \n",
    "            losses['train'][epoch] = f\"{batch_loss:.3f}\"\n",
    "            #progress bar \n",
    "            tr.set_postfix({\"epoch_num\":epoch,\n",
    "                            \"loss\":f\"{batch_loss:.10f}\"})\n",
    "            \n",
    "            with open(os.path.join(models_directory, \"loss.json\"), 'w') as f:\n",
    "                json.dump(losses, f)\n",
    "            \n",
    "            writer.add_scalar('Training loss', batch_loss, global_step=epoch+1)\n",
    "            # save models\n",
    "            if (epoch > 3 and epoch % 5 == 0):\n",
    "                torch.save(model, os.path.join(models_directory, f\"model_{epoch}\"))\n",
    "    torch.save(model, os.path.join(models_directory, f\"model_last_{epoch}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444e046",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 420541,
     "status": "ok",
     "timestamp": 1683757085355,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "4444e046",
    "outputId": "279d8873-1753-4cf1-9622-eaab0b82a39a"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "train(train_filepath = \"../data/resdsql_pre/preprocessed_dataset_train.json\",\n",
    "      batch_size = 2, #32\n",
    "      bert_hidden_size = 768,\n",
    "      t5_hidden_size = 512,\n",
    "      lr = 1e-4,\n",
    "      num_epochs = 1, #300\n",
    "      max_input_length = 43,\n",
    "      max_output_length = 127,\n",
    "      bert_model = 'bert-base-uncased',\n",
    "      t5_model = 't5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa55e8",
   "metadata": {
    "id": "bbaa55e8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21797c26",
   "metadata": {
    "id": "083df0db"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b6b910f",
   "metadata": {
    "id": "6b6b910f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (t5): T5ForConditionalGeneration(\n",
       "    (shared): Embedding(32128, 512)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 512)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 8)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-5): 5 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 512)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 8)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-5): 5 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       "  )\n",
       "  (linear): Linear(in_features=768, out_features=22016, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model class must be defined somewhere\n",
    "model_path = os.path.join(os.getcwd(), \"models/BERT_T5_lr0.0001_bs32_bert-base-uncased_t5-small_v3/model_40\")\n",
    "\n",
    "model2 = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "353361ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "dev_filepath = \"../data/resdsql_pre/preprocessed_dataset_test.json\"\n",
    "batch_size = 1\n",
    "max_input_length = 43\n",
    "bert_model = 'bert-base-uncased'\n",
    "t5_model = 't5-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c38bf753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_ids - torch.Size([1, 43])\n",
      "encoder_input_attention_mask - torch.Size([1, 43])\n",
      "bert_output - torch.Size([1, 768])\n",
      "t5_input - torch.Size([1, 22016]) - tensor([[ 0.0809, -0.0774,  0.0108,  ...,  0.0087, -0.0572, -0.2858]])\n",
      "t5_input - torch.Size([1, 1, 43, 512]) - tensor([[[[ 0.0809, -0.0774,  0.0108,  ..., -0.0286, -0.1612, -0.1826],\n",
      "          [-0.1978,  0.1926, -0.3448,  ..., -0.1101,  0.0199, -0.1727],\n",
      "          [-0.1943, -0.2248,  0.1437,  ..., -0.1916, -0.4217, -0.0047],\n",
      "          ...,\n",
      "          [-0.0615,  0.3262,  0.1987,  ..., -0.5110, -0.0623,  0.1278],\n",
      "          [ 0.1566,  0.0035, -0.1666,  ...,  0.1950,  0.0997,  0.0483],\n",
      "          [-0.0175,  0.2901, -0.1606,  ...,  0.0087, -0.0572, -0.2858]]]])\n",
      "decoder_input_ids - torch.Size([1, 1])\n",
      "decoder_attention_mask - torch.Size([1, 1])\n",
      "initial decoder_input_ids - tensor([[0]])\n",
      "t5_outputs logits - torch.Size([1, 1, 32128])\n",
      "next_token_logits - torch.Size([1, 32128])\n",
      "pred decoder_input_ids - tensor([[   0, 1738]])\n",
      "t5_outputs logits - torch.Size([1, 2, 32128])\n",
      "next_token_logits - torch.Size([1, 32128])\n",
      "pred decoder_input_ids - tensor([[   0, 1738, 3476]])\n",
      "t5_outputs logits - torch.Size([1, 3, 32128])\n",
      "next_token_logits - torch.Size([1, 32128])\n",
      "pred decoder_input_ids - tensor([[   0, 1738, 3476,   41]])\n",
      "t5_outputs logits - torch.Size([1, 4, 32128])\n",
      "next_token_logits - torch.Size([1, 32128])\n",
      "pred decoder_input_ids - tensor([[   0, 1738, 3476,   41, 1429]])\n",
      "t5_outputs logits - torch.Size([1, 5, 32128])\n",
      "next_token_logits - torch.Size([1, 32128])\n",
      "pred decoder_input_ids - tensor([[   0, 1738, 3476,   41, 1429,    3]])\n",
      "t5_outputs logits - torch.Size([1, 6, 32128])\n",
      "next_token_logits - torch.Size([1, 32128])\n",
      "pred decoder_input_ids - tensor([[   0, 1738, 3476,   41, 1429,    3,   61]])\n",
      "t5_outputs logits - torch.Size([1, 7, 32128])\n",
      "next_token_logits - torch.Size([1, 32128])\n",
      "pred decoder_input_ids - tensor([[   0, 1738, 3476,   41, 1429,    3,   61,   45]])\n",
      "t5_outputs logits - torch.Size([1, 8, 32128])\n",
      "next_token_logits - torch.Size([1, 32128])\n",
      "pred decoder_input_ids - tensor([[   0, 1738, 3476,   41, 1429,    3,   61,   45, 1588]])\n",
      "t5_outputs logits - torch.Size([1, 9, 32128])\n",
      "next_token_logits - torch.Size([1, 32128])\n",
      "pred decoder_input_ids - tensor([[   0, 1738, 3476,   41, 1429,    3,   61,   45, 1588,  834]])\n",
      "t5_outputs logits - torch.Size([1, 10, 32128])\n",
      "next_token_logits - torch.Size([1, 32128])\n",
      "pred decoder_input_ids - tensor([[   0, 1738, 3476,   41, 1429,    3,   61,   45, 1588,  834, 9476]])\n",
      "t5_outputs logits - torch.Size([1, 11, 32128])\n",
      "next_token_logits - torch.Size([1, 32128])\n",
      "pred decoder_input_ids - tensor([[   0, 1738, 3476,   41, 1429,    3,   61,   45, 1588,  834, 9476,  563]])\n",
      "t5_outputs logits - torch.Size([1, 12, 32128])\n",
      "next_token_logits - torch.Size([1, 32128])\n",
      "pred decoder_input_ids - tensor([[   0, 1738, 3476,   41, 1429,    3,   61,   45, 1588,  834, 9476,  563,\n",
      "           57]])\n",
      "t5_outputs logits - torch.Size([1, 13, 32128])\n",
      "next_token_logits - torch.Size([1, 32128])\n",
      "pred decoder_input_ids - tensor([[   0, 1738, 3476,   41, 1429,    3,   61,   45, 1588,  834, 9476,  563,\n",
      "           57, 2259]])\n",
      "t5_outputs logits - torch.Size([1, 14, 32128])\n",
      "next_token_logits - torch.Size([1, 32128])\n",
      "pred decoder_input_ids - tensor([[   0, 1738, 3476,   41, 1429,    3,   61,   45, 1588,  834, 9476,  563,\n",
      "           57, 2259,  834]])\n",
      "t5_outputs logits - torch.Size([1, 15, 32128])\n",
      "next_token_logits - torch.Size([1, 32128])\n",
      "pred decoder_input_ids - tensor([[   0, 1738, 3476,   41, 1429,    3,   61,   45, 1588,  834, 9476,  563,\n",
      "           57, 2259,  834,   23]])\n",
      "t5_outputs logits - torch.Size([1, 16, 32128])\n",
      "next_token_logits - torch.Size([1, 32128])\n",
      "pred decoder_input_ids - tensor([[   0, 1738, 3476,   41, 1429,    3,   61,   45, 1588,  834, 9476,  563,\n",
      "           57, 2259,  834,   23,   26]])\n",
      "t5_outputs logits - torch.Size([1, 17, 32128])\n",
      "next_token_logits - torch.Size([1, 32128])\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# start_time = time.time()\n",
    "\n",
    "# initialize tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model)\n",
    "\n",
    "# if isinstance(tokenizer, T5TokenizerFast):\n",
    "#     tokenizer.add_tokens([AddedToken(\" <=\"), AddedToken(\" <\")])\n",
    "\n",
    "dev_dataset = Text2SQLDataset(\n",
    "            dir_ = dev_filepath,\n",
    "            mode = \"train\")\n",
    "\n",
    "dev_dataloder = DataLoader(\n",
    "        dev_dataset, \n",
    "        batch_size = batch_size, \n",
    "        shuffle = False,\n",
    "        collate_fn = lambda x: x,\n",
    "        drop_last = False\n",
    "    )\n",
    "\n",
    "# initialize model\n",
    "\n",
    "model2.eval()\n",
    "predict_sqls = []\n",
    "# for batch in tqdm(dev_dataloder):\n",
    "for idx, batch in enumerate(dev_dataloder):\n",
    "    batch_inputs = [data[0] for data in batch]\n",
    "    batch_db_ids = [data[1] for data in batch]\n",
    "    batch_tc_original = [data[2] for data in batch]\n",
    "\n",
    "    tokenized_inputs = bert_tokenizer(batch_inputs,\n",
    "                                      add_special_tokens=True,\n",
    "                                      padding=\"max_length\", #True,\n",
    "                                      max_length=max_input_length,\n",
    "                                      #pad_to_max_length=True,\n",
    "                                      return_tensors='pt',\n",
    "                                      truncation=True)\n",
    "\n",
    "    encoder_input_ids = tokenized_inputs[\"input_ids\"].to(device)\n",
    "    encoder_input_attention_mask = tokenized_inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    print(f\"encoder_input_ids - {encoder_input_ids.size()}\")\n",
    "    print(f\"encoder_input_attention_mask - {encoder_input_attention_mask.size()}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_outputs = model2.predict(encoder_input_ids, encoder_input_attention_mask,\n",
    "                                       batch_size=1, t5_tokenizer=t5_tokenizer)\n",
    "\n",
    "#         model_outputs = model_outputs.view(len(batch_inputs), opt.num_return_sequences, model_outputs.shape[1])\n",
    "        \n",
    "#         predict_sqls += decode_sqls(\n",
    "#                                     opt.db_path, \n",
    "#                                     model_outputs, \n",
    "#                                     batch_db_ids, \n",
    "#                                     batch_inputs, \n",
    "#                                     tokenizer, \n",
    "#                                     batch_tc_original\n",
    "#                                     )\n",
    "    break\n",
    "\n",
    "\n",
    "# new_dir = \"/\".join(opt.output.split(\"/\")[:-1]).strip()\n",
    "# if new_dir != \"\":\n",
    "#     os.makedirs(new_dir, exist_ok = True)\n",
    "\n",
    "# # save results\n",
    "# with open(opt.output, \"w\", encoding = 'utf-8') as f:\n",
    "#     for pred in predict_sqls:\n",
    "#         f.write(pred + \"\\n\")\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(\"Text-to-SQL inference spends {}s.\".format(end_time-start_time))\n",
    "\n",
    "# if opt.mode == \"eval\":\n",
    "#     # initialize evaluator\n",
    "#     evaluator = EvaluateTool()\n",
    "#     evaluator.register_golds(opt.original_dev_filepath, opt.db_path)\n",
    "#     spider_metric_result = evaluator.evaluate(predict_sqls)\n",
    "#     print('exact_match score: {}'.format(spider_metric_result[\"exact_match\"]))\n",
    "#     print('exec score: {}'.format(spider_metric_result[\"exec\"]))\n",
    "\n",
    "#     return spider_metric_result[\"exact_match\"], spider_metric_result[\"exec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa42426d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'select count ( * ) from match_season group by competition_id'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebf57136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['find the number of matches in different competitions . '],\n",
       " ['select count ( * ) , competition from match group by competition'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs, batch_db_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e5efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
